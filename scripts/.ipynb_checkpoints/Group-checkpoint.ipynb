{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "ZHECHO :\n",
    "    1. function \"skewness\" applies ^3 is that correct?, additionally currently the skewness is not applied to the data (see code below)\n",
    "    2. counter added to gradient decent\n",
    "    3. Why values 0 and 1 are more suitable? The helpers use 1 and -1 as well as the competition. Trying linear regression with 1 and -1 values, doesn't work, diverges.\n",
    "    4. I think we are currently applying logistic regression, because of sigmoid function\n",
    "    5. Singular value decomposition doesn't work with the first load\n",
    "    6. Why do we need encoding?\n",
    "    7. The best method turns out to be mean imputation on the test (competition) set, maybe the outliers is the reason\n",
    "    8. Competition insights : Turns out that submitting only \"-1\" entries gives accuracy of 0.66, which means that 2/3 of the data is \"background\" and 1/3 \"signal\". Note that the f1-score for the submission was zero. Imputing the data using the mean value outputs an accuracy of 0.697, which is a slight improvement. F1-score is 0.634. Interestingly, even though the kmeans is more accurate on the validation set, it seems that the on the test data it scores less than 0.697. It seems that the method predicts that there are too many \"signals\" (at least 60k more than anticipated)\n",
    "    9. refactorization of the code will be done at the end, in case you create any implementation try to leave small comments to ensure readability of code\n",
    "    \n",
    "ANTHONY :\n",
    "    05/10/20 :\n",
    "        - is_cat() and build_poly() functions added.\n",
    "        - Small correction in outliers() function (typo: \"x_train\" -> \"x\").\n",
    "        - Putting less useful codes as comments.\n",
    "        - The comments below must be taken into account after\n",
    "          the basic problems have been solved. -> See ZHECHO's comments. \n",
    "        - Implementation of the build_poly() functions at the appropriate place (What is your opinion?).\n",
    "          Because the more we increase the dimensions, the more unstable the gradient descent is\n",
    "          (because of the change of scale). Should we 1. augment the data, 2. scale, 3. PCA?\n",
    "          Or maybe 1. PCA, 2. augment, 3. scale? ...\n",
    "        - We need to create a PCA() function that keeps only the useful sub-linear representation of our data.\n",
    "          In this way, we will have orthogonal dimensions and dimensionality reduction -> faster convergence.\n",
    "          \n",
    "ANTHONY :\n",
    "    06/10/20 :\n",
    "        - threshold() function added.\n",
    "        - Change of strategy : imputation, augmentation, scaling, PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries, functions and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from proj1_helpers import *\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sweetviz as sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_scaling(x):\n",
    "    mean_ = np.mean(x, axis=0)\n",
    "    x_scaled = x - mean_\n",
    "    std_ = np.std(x, axis=0)\n",
    "    x_scaled = x_scaled / std_\n",
    "    return x_scaled, mean_, std_\n",
    "\n",
    "\n",
    "def col_na_omit(x, tol=1):\n",
    "    index = sum(np.isnan(x))/(x.shape[0]) > tol\n",
    "    x = x[:, ~index]\n",
    "    return x\n",
    "\n",
    "\n",
    "def row_na_omit(y, x):\n",
    "    index = np.isnan(x).any(axis=1)\n",
    "    x_no_na = x[~index]\n",
    "    y_no_na = y[~index]\n",
    "    return y_no_na, x_no_na\n",
    "\n",
    "\n",
    "def outliers(y, x, clean_data, quantile=3):\n",
    "    print(f\"*****************************************************\")\n",
    "    print(f\"                                                     \")\n",
    "    print(f\"WARNING : Outliers must be removed before imputation.\")\n",
    "    print(f\"                                                     \")\n",
    "    print(f\"IGNORE : RuntimeWarning: invalid value encountered...\",\n",
    "          \"\\n\", \"This is due to NAs in the data set.\")\n",
    "    print(f\"                                                     \")\n",
    "    print(f\"*****************************************************\")\n",
    "    m = np.mean(clean_data, axis=0)\n",
    "    s = np.std(clean_data, axis=0)\n",
    "    lower = m - quantile * s\n",
    "    upper = m + quantile * s\n",
    "\n",
    "    for i in tqdm(range(clean_data.shape[1])):\n",
    "\n",
    "        index = clean_data[:, i] < lower[i]\n",
    "        clean_data = np.delete(clean_data, index*1, axis=0)\n",
    "\n",
    "        index = x[:, i] < lower[i]\n",
    "        x = np.delete(x, index*1, axis=0)\n",
    "        y = np.delete(y, index*1, axis=0)\n",
    "\n",
    "        index = clean_data[:, i] > upper[i]\n",
    "        clean_data = np.delete(clean_data, index*1, axis=0)\n",
    "\n",
    "        index = x[:, i] > upper[i]\n",
    "        x = np.delete(x, index*1, axis=0)\n",
    "        y = np.delete(y, index*1, axis=0)\n",
    "\n",
    "    return y, x, clean_data\n",
    "\n",
    "\n",
    "def mean_imputation(na_data, clean_data):\n",
    "    null, mean_x, std_x = gaussian_scaling(clean_data)\n",
    "    na_data = (na_data - mean_x)/std_x\n",
    "    na_data = np.nan_to_num(na_data, nan=0)\n",
    "    return(na_data)\n",
    "\n",
    "\n",
    "def median_imputation(na_data, clean_data):\n",
    "    null, mean_x, std_x = gaussian_scaling(clean_data)\n",
    "    na_data = (na_data - mean_x)/std_x\n",
    "    median = np.median(null, axis=0)\n",
    "    for i in range(na_data.shape[1]):\n",
    "        na_data[:, i] = np.nan_to_num(na_data[:, i], nan=median[i])\n",
    "    return(na_data)\n",
    "\n",
    "\n",
    "def skewness(x):\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    std_x = np.std(x, axis=0)\n",
    "    kurt = np.mean(((x - mean_x)/std_x)**3, axis=0)\n",
    "    return kurt\n",
    "\n",
    "\n",
    "def min_max_scaling(x):\n",
    "    min_ = np.min(x, axis=0)\n",
    "    max_ = np.max(x, axis=0)\n",
    "    x_scaled = (x - min_)/(max_ - min_)\n",
    "    return x_scaled, min_, max_\n",
    "\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    num_row = len(y)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    index_split = int(np.floor(ratio * num_row))\n",
    "    index_tr = indices[: index_split]\n",
    "    index_te = indices[index_split:]\n",
    "    # create split\n",
    "    x_tr = x[index_tr]\n",
    "    x_te = x[index_te]\n",
    "    y_tr = y[index_tr]\n",
    "    y_te = y[index_te]\n",
    "    return x_tr, x_te, y_tr, y_te\n",
    "\n",
    "\n",
    "def pairwise(p, q):\n",
    "    return np.sqrt(np.sum((p[:, np.newaxis, :]-q[np.newaxis, :, :])**2, axis=2))\n",
    "\n",
    "\n",
    "def random_sample(x, length):\n",
    "    num_row = x.shape[0]\n",
    "    indices = np.random.permutation(num_row)\n",
    "    sample = x[indices][:length]\n",
    "    return sample\n",
    "\n",
    "\n",
    "def stochastic_kmeans_imputation(na_data, clean_data, neighbors=10, length=1000):\n",
    "    clean_data, mean_x, std_x = gaussian_scaling(clean_data)\n",
    "    na_data = (na_data - mean_x)/std_x\n",
    "    sample = random_sample(x=clean_data, length=length)\n",
    "    for i in tqdm(range(na_data.shape[0])):\n",
    "        condition = np.isnan(na_data[i, :])\n",
    "        if len(condition) > 0:\n",
    "            index = np.where(condition)\n",
    "            candidate = np.delete(na_data[i], index)\n",
    "            neighborhood = np.delete(sample, index, axis=1)\n",
    "            distances = pairwise(candidate.reshape(\n",
    "                (1, len(candidate))), neighborhood)\n",
    "            nearest_index = np.argsort(distances)[0][:neighbors]\n",
    "            na_data[i, index] = np.mean(sample[nearest_index], axis=0)[index]\n",
    "    return(na_data)\n",
    "\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss = (1/len(y)) * y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    return (1/len(y))*np.squeeze(- loss)\n",
    "\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    y = np.array(y)\n",
    "    tx = np.array(tx)\n",
    "    w = np.array(w)\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    grad = tx.T.dot(pred - y)\n",
    "    return (1/len(y))*grad\n",
    "\n",
    "\n",
    "def gradient_descent(y, tx, w, max_iter, gamma, eps=1e-4):\n",
    "    counter = 0\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    b = np.linalg.norm(grad)\n",
    "    while b > eps:\n",
    "        counter += 1\n",
    "        grad = calculate_gradient(y, tx, w)\n",
    "        w -= gamma * grad\n",
    "        b = np.linalg.norm(grad)\n",
    "        print(f\"Gradient norm = {b}\\r\", end=\"\")\n",
    "        if counter == max_iter:\n",
    "            print(\"reached max_iter\")\n",
    "            break\n",
    "    return calculate_loss(y, tx, w), w, np.linalg.norm(grad)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, w, batch_size, max_iter, gamma, eps=1e-4):\n",
    "    counter = 0\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    b = np.linalg.norm(grad)\n",
    "    while b > eps:\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad = calculate_gradient(y_batch, tx_batch, w)\n",
    "            b = np.linalg.norm(grad)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            print(f\"Gradient norm = {b}\\r\", end=\"\")\n",
    "            counter += 1\n",
    "        if counter == max_iter:\n",
    "            print(\"reached max_iter\")\n",
    "            break\n",
    "    return calculate_loss(y, tx, w), w, np.linalg.norm((1/len(y))*calculate_gradient(y, tx, w))\n",
    "\n",
    "\n",
    "def is_cat(x, length=10):\n",
    "    \"\"\"Check if an array is categorical\"\"\"\n",
    "    boolean_index = list([])\n",
    "    if x.shape == (len(x),):\n",
    "        if len(set(x)) < length:\n",
    "            boolean = True\n",
    "        else:\n",
    "            boolean = False\n",
    "            boolean_index.append(boolean)\n",
    "    else:\n",
    "        for i in range(x.shape[1]):\n",
    "            if len(set(x[:, i])) < length:\n",
    "                boolean = True\n",
    "            else:\n",
    "                boolean = False\n",
    "            boolean_index.append(boolean)\n",
    "    return np.array(boolean_index)\n",
    "\n",
    "\n",
    "def build_poly(x, degree, pairwise_interaction=True, intercept=False):\n",
    "    null, x_rNa = row_na_omit(x[:, 0], x)\n",
    "    cat_index = is_cat(x_rNa)\n",
    "    categorical_variables = x[:, cat_index]\n",
    "    continuous_variables = x[:, ~cat_index]\n",
    "    augmented_x = continuous_variables\n",
    "    if degree > 1:\n",
    "        for i in range(2, degree+1):\n",
    "            augmented_x = np.c_[augmented_x, np.power(continuous_variables, i)]\n",
    "    if pairwise_interaction:\n",
    "        for j in tqdm(range(continuous_variables.shape[1])):\n",
    "            for k in range(continuous_variables.shape[1]):\n",
    "                if j >= k:\n",
    "                    continue\n",
    "                else:\n",
    "                    augmented_x = np.c_[augmented_x, np.multiply(\n",
    "                        continuous_variables[:, j], continuous_variables[:, k])]\n",
    "    if intercept:\n",
    "        inter = np.ones((x.shape[0], 1))\n",
    "        augmented_x = np.c_[np.ones((x.shape[0], 1)), augmented_x]\n",
    "    augmented_x = np.c_[augmented_x, categorical_variables]\n",
    "    return augmented_x\n",
    "\n",
    "def threshold(y, fitted_probabilities, step = 0.01):\n",
    "    \"\"\"find the best threshold for classification\"\"\"\n",
    "    candidates = np.arange(0.2, 0.8, step)\n",
    "    thresholds = list([])\n",
    "    accuracies = list([])\n",
    "    for i in tqdm(candidates):\n",
    "        prediction = (fitted_probabilities>i)*1\n",
    "        accuracy = 1 - sum(np.abs(prediction - y))/len(y)\n",
    "        thresholds.append(i)\n",
    "        accuracies.append(accuracy)\n",
    "    index = accuracies.index(max(accuracies))\n",
    "    return thresholds[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, x_train, ids_train = load_csv_data(\"data/train.csv\")\n",
    "x_train[np.where(x_train == -999)] = np.nan\n",
    "y_train[np.where(y_train == -1)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a clean dataset without missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rNA, x_train_rNA = row_na_omit(y_train, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be done on train and test.\n",
    "x_train = col_na_omit(x_train, tol = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_te, y_train, y_te = split_data(x_train, y_train, 0.8, seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************\n",
      "                                                     \n",
      "WARNING : Outliers must be removed before imputation.\n",
      "                                                     \n",
      "IGNORE : RuntimeWarning: invalid value encountered... \n",
      " This is due to NAs in the data set.\n",
      "                                                     \n",
      "*****************************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcf098cf450474685e57dd70b1ec4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-80-12addd892c12>:41: RuntimeWarning: invalid value encountered in less\n",
      "  index = x[:, i] < lower[i]\n",
      "<ipython-input-80-12addd892c12>:48: RuntimeWarning: invalid value encountered in greater\n",
      "  index = x[:, i] > upper[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Can't be done on test set.\n",
    "y_train, x_train, x_train_rNA = outliers(y_train, x_train, x_train_rNA, quantile = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian scaling, imputation and skewness analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_mean = mean_imputation(x_train, x_train_rNA)\n",
    "# x_te_mean = mean_imputation(x_te, x_train_rNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originaly, median_imputation was scaling and imputing. But here I wanted to impute on the original scale.\n",
    "# Hence, I define a knew median_imputation() function.\n",
    "def median_imputation(na_data, clean_data):\n",
    "    median = np.median(clean_data, axis=0)\n",
    "    for i in range(na_data.shape[1]):\n",
    "        na_data[:, i] = np.nan_to_num(na_data[:, i], nan=median[i])\n",
    "    return(na_data)\n",
    "\n",
    "# Imputation on the original scale\n",
    "x_train_median = median_imputation(x_train, x_train_rNA)\n",
    "x_te_median = median_imputation(x_te, x_train_rNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic kmeans imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = stochastic_kmeans_imputation(na_data = x_train, clean_data = x_train_rNA, length=1000)\n",
    "# x_te = stochastic_kmeans_imputation(na_data = x_te, clean_data = x_train_rNA, length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f138a7f13a4fb8993e6d5a649133e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015263602c9c478789db7ec6e2e1f36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85333fe5d5604b3abd6c3787176c98a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Augmentation on the original scale\n",
    "x_train_augmented = build_poly(x = x_train_median, degree = 4, pairwise_interaction = True)\n",
    "x_te_augmented = build_poly(x = x_te_median, degree = 4, pairwise_interaction = True)\n",
    "x_train_rNA_augmented = build_poly(x = x_train_rNA, degree = 4, pairwise_interaction = True)\n",
    " \n",
    "# Scaling\n",
    "null, mean_, std_ = gaussian_scaling(x_train_rNA_augmented)\n",
    "x_train_augmented = (x_train_augmented - mean_) / std_\n",
    "x_te_augmented = (x_te_augmented - mean_) / std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under normality assumption, Gaussian scaling makes sense.\n",
    "# print(np.min(x_train_augmented, axis=0), \"\\n\", \"\\n\", np.max(x_train_augmented, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness(x_train_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density example, symmetry seems to be \"good enough\".\n",
    "# pd.DataFrame(x_train_augmented.T[0]).plot(kind='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda = sv.analyze(pd.concat([pd.DataFrame({\"Prediction\" : y_train}), pd.DataFrame(x_train)], axis=1))\n",
    "# eda.show_html('eda.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depency structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We need SVD for fast convergence.\n",
    "# df = pd.DataFrame(x_train_augmented)\n",
    "# f = plt.figure(figsize=(10, 10))\n",
    "# plt.matshow(df.corr(), fignum=f.number)\n",
    "# plt.xticks(range(df.shape[1]), df.columns, fontsize=10, rotation=45)\n",
    "# plt.yticks(range(df.shape[1]), df.columns, fontsize=10)\n",
    "# cb = plt.colorbar()\n",
    "# cb.ax.tick_params(labelsize=10)\n",
    "# plt.title('Correlation Matrix', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigen(train, test, var=99.99999999):\n",
    "    \"\"\"Simple eigenvalue decomposition and sorting by importance\"\"\"\n",
    "    A = np.cov(train, rowvar=False)\n",
    "    eigenValues, eigenVectors = np.linalg.eig(A)\n",
    "    idx = np.argsort(-eigenValues)\n",
    "    eigenValues = eigenValues[idx]\n",
    "    eigenVectors = eigenVectors[:,idx]\n",
    "    candidates = (np.cumsum(eigenValues)/np.sum(eigenValues))\n",
    "    index = np.where(candidates>=(var/100))[0][0]\n",
    "    train = train.dot(eigenVectors.real[:,:(index+1)])\n",
    "    test = test.dot(eigenVectors.real[:,:(index+1)])\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_changed, x_te_changed = eigen(x_train_augmented, x_te_augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm = 0.049936753909936824\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-12addd892c12>:154: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = (1/len(y)) * y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n"
     ]
    }
   ],
   "source": [
    "loss, w, grad_norm = gradient_descent(y_train,\n",
    "                                      tx = x_train_changed,\n",
    "                                      w = np.zeros(x_train_changed.shape[1]),\n",
    "                                      max_iter = 10000,\n",
    "                                      gamma = 0.05,\n",
    "                                      eps = 0.05)\n",
    "# print(loss,\"\\n\", \"\\n\", w,\"\\n\", \"\\n\", grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.12368931e-02,  6.31962122e+00, -9.85474993e-01, -4.41711867e+00,\n",
       "       -2.31964105e-01,  1.92801774e+00,  6.22000185e-01,  2.41535190e+00,\n",
       "       -2.84497235e+00,  3.56621029e+00, -5.81616415e-02, -6.11822903e+00,\n",
       "       -9.56721901e-01,  2.86308591e-01,  3.19195370e+00, -1.31378786e+00,\n",
       "       -1.24394327e+00, -4.46652199e+00,  1.82507701e+00, -1.26792463e+00,\n",
       "        2.79877354e-01,  3.29696616e-01,  1.80424030e+00,  4.26615105e-01,\n",
       "       -5.38809312e-01, -1.68310581e+00, -9.79823970e-01, -7.87091314e-01,\n",
       "        9.01693295e-01, -2.68765467e-01, -3.63538520e-01, -3.65210178e-01,\n",
       "        5.13204137e-01, -5.10354372e-02,  1.84854460e+00, -1.32219848e-01,\n",
       "        5.02382846e-01, -1.87381880e+00,  3.56364564e-01,  1.08724284e+00,\n",
       "       -1.19476918e+00,  5.73091930e-02, -3.78814406e-01,  6.96758818e-02,\n",
       "        7.14290354e-01,  3.37947578e-01,  5.20799064e-01,  7.44133838e-01,\n",
       "       -9.94260997e-02, -7.81677592e-01, -5.87911140e-02,  6.32728890e-01,\n",
       "        2.61945797e+00, -1.76168432e+00, -2.95255440e-01, -3.15662589e-01,\n",
       "       -3.81380181e+00,  7.83274608e-02,  3.91876207e-01, -1.86165756e+00,\n",
       "        5.86447970e-01, -1.70980712e-01, -4.51765976e-01,  2.40257578e-01,\n",
       "        3.61304435e-01, -6.57770683e-02, -2.12620314e-01, -7.01248794e-01,\n",
       "       -6.47161173e-01,  7.45775336e-01,  2.58575116e-02,  6.58760070e-01,\n",
       "       -7.67416451e-01,  7.66475053e-01,  1.03041544e+00, -4.66251505e-01,\n",
       "       -6.31900631e-01,  1.43157617e+00, -7.22096402e-01,  9.50421968e-02,\n",
       "       -4.49278441e-01, -4.27946582e-01,  1.86587202e+00,  1.30990449e-01,\n",
       "       -1.26264857e-01, -6.61836545e-01, -4.03388610e-03, -2.18079690e-01,\n",
       "        2.63489815e-01,  1.35975397e-01,  6.26336203e-01,  1.31476431e+00,\n",
       "       -2.28389147e-01, -1.35692734e+00,  5.49807755e-01, -3.29774156e-01,\n",
       "       -1.02882836e+00, -9.18750578e-01, -1.48775488e+00,  1.07356311e+00,\n",
       "       -5.62899192e-01,  1.55252852e+00, -6.88331668e-01,  4.84001640e-01,\n",
       "       -1.42902457e-01, -4.85784190e-01, -5.08122587e-01,  1.25902033e+00,\n",
       "       -2.14997758e+00, -8.71665437e-01, -1.76582757e-01, -1.54315738e+00,\n",
       "       -1.09781657e+00, -1.11785409e+00, -1.65455990e+00, -1.59478563e+00,\n",
       "       -7.39960701e-01,  1.76594732e-01, -2.08913627e-01,  1.13430445e-01,\n",
       "       -5.63734000e-01,  9.88431248e-01, -1.67946966e-01, -1.31131788e-01,\n",
       "        1.91526941e-01, -1.16318135e-01, -7.74642987e-01,  6.47461853e-02,\n",
       "       -7.97590003e-01, -3.01248395e-01, -7.77344207e-02, -4.28111409e-01,\n",
       "        1.26727025e-01, -8.09175865e-01, -1.05108003e-01, -3.65871164e-01,\n",
       "        1.60287370e+00, -1.75346415e+00,  6.19977663e-01, -8.05198530e-01,\n",
       "        8.94895066e-01,  3.03655732e-01, -8.45899243e-02,  5.80593449e-01,\n",
       "        2.85418621e-01,  2.45030514e-01, -7.00455449e-01,  1.01516310e-01,\n",
       "       -6.63283972e-01,  4.11900809e-01, -2.75813052e-01, -7.82077835e-02,\n",
       "        3.78006702e-01, -1.24412312e-01,  6.33984841e-01,  3.64683145e-01,\n",
       "       -3.49805596e-01,  1.35421841e-01,  6.31658777e-01, -2.14480753e-01,\n",
       "       -1.42305511e-01, -7.33574418e-01, -7.27691450e-01, -3.83403256e-01,\n",
       "        1.24085117e-01,  3.49598952e-01, -1.79805624e-01,  2.73560963e-02,\n",
       "        4.44978332e-01,  3.68261582e-01,  2.50377978e-01, -9.75525140e-01,\n",
       "       -5.66900336e-02, -5.72745377e-01,  7.85696917e-03, -2.77263898e-02,\n",
       "        2.98533640e-01,  2.75332646e-01, -1.82052250e-01,  8.69689930e-01,\n",
       "       -2.94899299e-02,  4.69154706e-01, -1.14628126e-01,  2.55566886e-01,\n",
       "        2.66715570e-01, -3.26335310e-01,  3.76974479e-01, -2.31851149e-01,\n",
       "       -1.37688589e-01, -1.25772800e-02, -3.93495676e-02,  4.04027502e-02,\n",
       "       -4.16104322e-02, -1.14351021e-01, -2.99827696e-01, -1.91258589e-01,\n",
       "        2.05436740e-01, -1.27234261e-01,  1.05120694e-02, -4.90202632e-01,\n",
       "       -2.66741807e-01,  1.03308076e-01,  6.48658969e-02,  2.20483786e-01,\n",
       "        1.31606871e-01, -3.27846388e-01,  5.16892324e-01, -4.86673436e-01,\n",
       "       -3.00731330e-01,  5.93976150e-01, -7.58955830e-03,  3.46469555e-02,\n",
       "        3.56308042e-02,  7.85389826e-02,  5.21108309e-01, -1.89699364e-01,\n",
       "       -1.71466203e-01, -1.79319453e-01, -6.06422230e-02,  3.50187410e-01,\n",
       "       -2.68204777e-01,  1.71780644e-02,  1.55809513e-01,  2.47573693e-01,\n",
       "        2.12777234e-01, -7.96892616e-02, -6.71572459e-02,  2.39892929e-01,\n",
       "        1.79138759e-01, -2.73548762e-01,  2.00674253e-01,  2.98683218e-02,\n",
       "        3.04185570e-02,  2.72858007e-01,  1.83909266e-01, -3.09786070e-01,\n",
       "       -1.34335755e-01,  1.32851863e-01, -1.07770614e-01,  7.49186673e-02,\n",
       "       -4.52669797e-02, -1.46744745e-01,  1.56579489e-01,  4.14259961e-03,\n",
       "       -2.73642721e-02, -3.45107454e-03, -2.37979840e-01, -1.36759613e-01,\n",
       "       -1.99187732e-01,  7.32385134e-02,  2.73699261e-01,  4.39924536e-01,\n",
       "        4.76854939e-02,  1.72622018e-01,  9.81980430e-02,  4.44289336e-02,\n",
       "        2.46460879e-01, -3.76244523e-01,  2.51996540e-01,  1.99981607e-02,\n",
       "       -9.01149159e-02,  8.60968042e-02,  4.17923640e-01, -4.32349285e-01,\n",
       "       -1.18790262e-01,  3.35497663e-02,  4.46342440e-01, -4.08461097e-02,\n",
       "       -3.65048915e-01, -8.36410990e-02,  5.50919709e-01, -2.41287397e-02,\n",
       "        2.12459901e-01,  1.10980664e-01, -3.17525687e-01, -1.66932838e-01,\n",
       "        9.21020405e-02,  2.63597961e-01, -2.09616246e-01,  3.18502998e-01,\n",
       "        1.08706947e-01, -3.10926540e-01,  1.13213110e-01,  1.84602248e-01,\n",
       "       -8.67888151e-02,  1.97544144e-01,  1.57800081e-01, -5.51370883e-02,\n",
       "        1.08103313e-01,  8.87607063e-02,  1.45262950e-01,  1.61474160e-01,\n",
       "       -1.78049314e-01,  4.70724733e-02,  3.97609093e-02,  2.15872541e-01,\n",
       "       -5.34482482e-03,  3.09661930e-01,  1.59981046e-01,  2.42853637e-01,\n",
       "       -2.46524550e-01, -5.85878478e-02, -1.54946963e-01,  1.57486330e-01,\n",
       "        1.32317749e-01,  9.61654061e-02,  1.40574997e-01, -1.34818134e-02,\n",
       "       -1.88056240e-01,  1.67959438e-02,  1.14329741e-01, -1.82728779e-01,\n",
       "       -1.60621569e-01, -1.25492677e-01, -3.16696637e-01,  1.43357866e-01,\n",
       "        1.44042262e-01,  5.21228620e-02,  1.18493556e-01,  2.85040016e-01,\n",
       "        3.40513229e-02, -1.30759095e-01,  1.57358611e-01, -2.00376283e-01,\n",
       "        1.90340139e-01,  4.54707951e-02, -2.09732298e-02,  8.27386718e-02,\n",
       "       -2.15666971e-01,  7.29722738e-02,  9.52992148e-02, -5.42538492e-02,\n",
       "        1.00037704e-01, -9.65472907e-02, -1.24802369e-01,  1.00639906e-01,\n",
       "       -3.02621336e-02, -1.69669358e-01,  1.40898953e-01, -2.52023350e-01,\n",
       "        1.08292767e-01,  2.18880782e-01, -1.23583421e-01,  4.95941396e-03,\n",
       "       -2.09897289e-01, -3.23443475e-01,  1.27706616e-01, -7.04229376e-02,\n",
       "        7.81711396e-02,  1.73179065e-02, -1.70534166e-02, -9.52441728e-02,\n",
       "       -3.78093998e-02,  2.67611664e-02,  1.02171091e-01, -1.29401110e-02,\n",
       "        2.49980794e-02,  4.62524650e-02, -1.02265403e-01,  3.18660317e-01,\n",
       "        4.06604374e-02,  2.27774888e-01,  7.44661344e-02, -3.57598271e-02,\n",
       "       -1.22775589e-02,  1.02029154e-01, -1.23419491e-02,  3.77768978e-02,\n",
       "       -1.51425043e-01, -6.43910987e-02, -7.23358718e-02,  1.08595626e-02,\n",
       "        3.96396523e-02,  1.40815096e-01, -1.27525899e-01,  1.02909028e-01,\n",
       "       -4.43154474e-02, -4.74901090e-02,  9.43932079e-02,  4.71723737e-02,\n",
       "        5.17927154e-02,  7.55832296e-02,  2.05316663e-02,  9.50959322e-03,\n",
       "        2.93898640e-02,  8.88169219e-02, -4.00584512e-02, -9.43529367e-02,\n",
       "        2.33647433e-02,  3.64324592e-02, -7.60896253e-03,  2.70629703e-02,\n",
       "       -5.72398786e-02,  7.60257981e-02, -7.72718882e-03,  2.25641769e-02,\n",
       "       -2.82821243e-02, -9.00130691e-03,  8.60286512e-02,  1.32702126e-02,\n",
       "        4.04401193e-02,  8.57831046e-03,  6.29748597e-02, -1.76652290e-01,\n",
       "       -5.49440995e-02,  4.10840772e-02, -8.92026691e-03, -6.23520822e-02,\n",
       "       -8.83256538e-02, -5.32226744e-02, -4.63166658e-02, -9.96919039e-02,\n",
       "       -1.18326674e-01, -6.04374685e-02, -7.87657467e-02,  2.51101400e-02,\n",
       "        5.13866917e-02, -1.14589238e-01, -2.85716692e-02, -5.16493392e-02,\n",
       "       -3.74821767e-02,  6.71218902e-02, -1.58631602e-02,  5.63436039e-02,\n",
       "        5.81012310e-02, -3.71266063e-04, -2.46140412e-02,  7.06570707e-03,\n",
       "        5.24122381e-03, -4.63567296e-02,  3.86070788e-02, -3.75150410e-02,\n",
       "       -3.73850638e-02, -4.07921706e-02, -5.24676693e-02, -9.50740866e-03,\n",
       "        1.32734688e-02, -3.58485669e-02, -5.76620055e-03, -1.16757635e-01,\n",
       "        3.45381771e-02, -8.07847436e-04, -5.17639685e-03,  5.77269912e-03,\n",
       "       -2.74992283e-03, -2.87381077e-02,  1.79507266e-02,  1.12185823e-02,\n",
       "        6.41642919e-04, -5.78075031e-05,  2.95835758e-02, -9.26171180e-03,\n",
       "        2.07393502e-02,  3.43656365e-02,  3.46352444e-02,  1.99971299e-02,\n",
       "       -1.53345553e-02, -5.63886203e-03, -9.70372832e-03, -2.32909419e-03,\n",
       "       -3.88792313e-02, -4.82699192e-02,  2.45522959e-03, -1.32069051e-03,\n",
       "        2.79016106e-03, -2.02622186e-02,  2.56586053e-02,  2.78196667e-02,\n",
       "        2.08830473e-02,  1.18799701e-02,  3.97364353e-03,  2.61551032e-03,\n",
       "       -1.66473867e-02,  5.06037171e-02,  1.60131976e-02, -1.63501401e-02,\n",
       "       -1.54929850e-02,  4.28593390e-04,  3.82165312e-03,  3.71307396e-03,\n",
       "       -9.14374050e-03, -2.23245651e-03,  1.33648880e-03, -1.02736722e-03,\n",
       "       -1.42981353e-03, -3.14621972e-03, -7.84127560e-03, -8.44511220e-04,\n",
       "        1.88691589e-03,  3.21668569e-03])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_changed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-dd03cae3aa87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstochastic_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train_changed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_changed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-80-12addd892c12>\u001b[0m in \u001b[0;36mstochastic_gradient_descent\u001b[1;34m(y, tx, w, batch_size, max_iter, gamma, eps)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m             \u001b[1;31m# compute a stochastic gradient and loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-12addd892c12>\u001b[0m in \u001b[0;36mbatch_iter\u001b[1;34m(y, tx, batch_size, num_batches, shuffle)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mshuffle_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mshuffled_tx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, w, grad_norm = stochastic_gradient_descent(y_train, x_train_changed, w = np.zeros(x_train_changed.shape[1]), batch_size=10000, max_iter=10000, gamma=0.05, eps=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387abf3755b54197a45d55c201e1a5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In sample performance : 0.7740596238495399\n"
     ]
    }
   ],
   "source": [
    "thresh = threshold(y_train, sigmoid(x_train_changed@w))\n",
    "pred = sigmoid(x_train_changed@w)\n",
    "pred = (pred>thresh)*1\n",
    "print(f\"In sample performance : {1 - sum(np.abs(pred - y_train))/len(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of sample performance : 0.7749\n"
     ]
    }
   ],
   "source": [
    "pred = sigmoid(x_te_changed@w)\n",
    "pred = (pred>thresh)*1\n",
    "print(f\"Out of sample performance : {1 - sum(np.abs(pred - y_te))/len(y_te)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recompute on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y , x, ids = load_csv_data(\"data/train.csv\")\n",
    "x[np.where(x == -999)] = np.nan\n",
    "y[np.where(y == -1)] = 0\n",
    "\n",
    "y_rNA, x_rNA = row_na_omit(y, x)\n",
    "\n",
    "# y, x, x_rNA = outliers(y, x, x_rNA, quantile = 3)\n",
    "\n",
    "x_median = median_imputation(x, x_rNA)\n",
    "\n",
    "x_augmented = build_poly(x = x_median, degree = 5, pairwise_interaction = True)\n",
    "x_rNA_augmented = build_poly(x = x_rNA, degree = 5, pairwise_interaction = True)\n",
    "\n",
    "\n",
    "null, mean_, std_ = gaussian_scaling(x_rNA_augmented)\n",
    "x_augmented = (x_augmented - mean_) / std_\n",
    "\n",
    "cov = np.cov(x_augmented, rowvar=False)\n",
    "eigenvalues, cov_eigenvectors = eigen(cov)\n",
    "\n",
    "x_augmented_changed = x_augmented.dot(cov_eigenvectors.real[:,sum(eigenvalues<1e-2):len(eigenvalues)])\n",
    "\n",
    "loss, w, grad_norm = gradient_descent(y,\n",
    "                                      tx = x_augmented_changed,\n",
    "                                      w = np.zeros(x_augmented_changed.shape[1]),\n",
    "                                      max_iter = 10,\n",
    "                                      gamma = 0.05,\n",
    "                                      eps = 1e-4)\n",
    "\n",
    "thresh = threshold(y, sigmoid(x_augmented_changed@w))\n",
    "pred = sigmoid(x_augmented_changed@w)\n",
    "pred = (pred>thresh)*1\n",
    "print(f\"In sample performance : {1 - sum(np.abs(pred - y))/len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test set\n",
    "y_pred, x_test, ids_test = load_csv_data(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure to apply the same imputaion changes to the test set\n",
    "x_test[np.where(x_test == -999)] = np.nan\n",
    "x_test = median_imputation(x_test, x_train_rNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying singular value decomposition as well (not sure if correct)\n",
    "x_test_augmented = build_poly(x = x_test, degree = 5, pairwise_interaction = True)\n",
    "x_test_augmented = (x_test_augmented - mean_) / std_\n",
    "x_test_changed = x_test_augmented.dot(cov_eigenvectors.real[:,sum(eigenvalues<1e-2):len(eigenvalues)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create prediction\n",
    "y_pred = sigmoid(x_test_changed@w)\n",
    "y_pred = (y_pred>thresh)*1\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert vectors for submition\n",
    "y_pred[np.where(y_pred == 0)] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observe the distribution of the classes\n",
    "print(len(y_pred[np.where(y_pred == 1)])/len(y_pred[np.where(y_pred == -1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, \"anthony_submission_06_10_5.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
