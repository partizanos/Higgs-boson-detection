{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from proj1_helpers import *\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sweetviz as sv\n",
    "\n",
    "def gaussian_scaling(x):\n",
    "    mean_ = np.mean(x, axis=0)\n",
    "    x_scaled = x - mean_\n",
    "    std_ = np.std(x, axis=0)\n",
    "    x_scaled = x_scaled / std_\n",
    "    return x_scaled, mean_, std_\n",
    "\n",
    "\n",
    "def col_na_omit(x, tol=1):\n",
    "    index = sum(np.isnan(x))/(x.shape[0]) > tol\n",
    "    x = x[:, ~index]\n",
    "    return x\n",
    "\n",
    "\n",
    "def row_na_omit(y, x):\n",
    "    index = np.isnan(x).any(axis=1)\n",
    "    x_no_na = x[~index]\n",
    "    y_no_na = y[~index]\n",
    "    return y_no_na, x_no_na\n",
    "\n",
    "\n",
    "def outliers(y, x, clean_data, quantile=3):\n",
    "    print(f\"*****************************************************\")\n",
    "    print(f\"                                                     \")\n",
    "    print(f\"WARNING : Outliers must be removed before imputation.\")\n",
    "    print(f\"                                                     \")\n",
    "    print(f\"IGNORE : RuntimeWarning: invalid value encountered...\",\n",
    "          \"\\n\", \"This is due to NAs in the data set.\")\n",
    "    print(f\"                                                     \")\n",
    "    print(f\"*****************************************************\")\n",
    "    m = np.mean(clean_data, axis=0)\n",
    "    s = np.std(clean_data, axis=0)\n",
    "    lower = m - quantile * s\n",
    "    upper = m + quantile * s\n",
    "\n",
    "    for i in tqdm(range(clean_data.shape[1])):\n",
    "\n",
    "        index = clean_data[:, i] < lower[i]\n",
    "        clean_data = np.delete(clean_data, index*1, axis=0)\n",
    "\n",
    "        index = x[:, i] < lower[i]\n",
    "        x = np.delete(x, index*1, axis=0)\n",
    "        y = np.delete(y, index*1, axis=0)\n",
    "\n",
    "        index = clean_data[:, i] > upper[i]\n",
    "        clean_data = np.delete(clean_data, index*1, axis=0)\n",
    "\n",
    "        index = x[:, i] > upper[i]\n",
    "        x = np.delete(x, index*1, axis=0)\n",
    "        y = np.delete(y, index*1, axis=0)\n",
    "\n",
    "    return y, x, clean_data\n",
    "\n",
    "\n",
    "def mean_imputation(na_data, clean_data):\n",
    "    null, mean_x, std_x = gaussian_scaling(clean_data)\n",
    "    na_data = (na_data - mean_x)/std_x\n",
    "    na_data = np.nan_to_num(na_data, nan=0)\n",
    "    return(na_data)\n",
    "\n",
    "\n",
    "def median_imputation(na_data, clean_data):\n",
    "    null, mean_x, std_x = gaussian_scaling(clean_data)\n",
    "    na_data = (na_data - mean_x)/std_x\n",
    "    median = np.median(null, axis=0)\n",
    "    for i in range(na_data.shape[1]):\n",
    "        na_data[:, i] = np.nan_to_num(na_data[:, i], nan=median[i])\n",
    "    return(na_data)\n",
    "\n",
    "\n",
    "def skewness(x):\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    std_x = np.std(x, axis=0)\n",
    "    kurt = np.mean(((x - mean_x)/std_x)**3, axis=0)\n",
    "    return kurt\n",
    "\n",
    "\n",
    "def min_max_scaling(x):\n",
    "    min_ = np.min(x, axis=0)\n",
    "    max_ = np.max(x, axis=0)\n",
    "    x_scaled = (x - min_)/(max_ - min_)\n",
    "    return x_scaled, min_, max_\n",
    "\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    num_row = len(y)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    index_split = int(np.floor(ratio * num_row))\n",
    "    index_tr = indices[: index_split]\n",
    "    index_te = indices[index_split:]\n",
    "    # create split\n",
    "    x_tr = x[index_tr]\n",
    "    x_te = x[index_te]\n",
    "    y_tr = y[index_tr]\n",
    "    y_te = y[index_te]\n",
    "    return x_tr, x_te, y_tr, y_te\n",
    "\n",
    "\n",
    "def pairwise(p, q):\n",
    "    return np.sqrt(np.sum((p[:, np.newaxis, :]-q[np.newaxis, :, :])**2, axis=2))\n",
    "\n",
    "\n",
    "def random_sample(x, length):\n",
    "    num_row = x.shape[0]\n",
    "    indices = np.random.permutation(num_row)\n",
    "    sample = x[indices][:length]\n",
    "    return sample\n",
    "\n",
    "\n",
    "def stochastic_kmeans_imputation(na_data, clean_data, neighbors=10, length=1000):\n",
    "    clean_data, mean_x, std_x = gaussian_scaling(clean_data)\n",
    "    na_data = (na_data - mean_x)/std_x\n",
    "    sample = random_sample(x=clean_data, length=length)\n",
    "    for i in tqdm(range(na_data.shape[0])):\n",
    "        condition = np.isnan(na_data[i, :])\n",
    "        if len(condition) > 0:\n",
    "            index = np.where(condition)\n",
    "            candidate = np.delete(na_data[i], index)\n",
    "            neighborhood = np.delete(sample, index, axis=1)\n",
    "            distances = pairwise(candidate.reshape(\n",
    "                (1, len(candidate))), neighborhood)\n",
    "            nearest_index = np.argsort(distances)[0][:neighbors]\n",
    "            na_data[i, index] = np.mean(sample[nearest_index], axis=0)[index]\n",
    "    return(na_data)\n",
    "\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss = (1/len(y)) * y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    return (1/len(y))*np.squeeze(- loss)\n",
    "\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    y = np.array(y)\n",
    "    tx = np.array(tx)\n",
    "    w = np.array(w)\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    grad = tx.T.dot(pred - y)\n",
    "    return (1/len(y))*grad\n",
    "\n",
    "\n",
    "def gradient_descent(y, tx, w, max_iter, gamma, eps=1e-4):\n",
    "    counter = 0\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    b = np.linalg.norm(grad)\n",
    "    while b > eps:\n",
    "        counter += 1\n",
    "        grad = calculate_gradient(y, tx, w)\n",
    "        w -= gamma * grad\n",
    "        b = np.linalg.norm(grad)\n",
    "        print(f\"Gradient norm = {b}\\r\", end=\"\")\n",
    "        if counter == max_iter:\n",
    "            print(\"reached max_iter\")\n",
    "            break\n",
    "    return calculate_loss(y, tx, w), w, np.linalg.norm(grad)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, w, batch_size, max_iter, gamma, eps=1e-4):\n",
    "    counter = 0\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    b = np.linalg.norm(grad)\n",
    "    while b > eps:\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad = calculate_gradient(y_batch, tx_batch, w)\n",
    "            b = np.linalg.norm(grad)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            #print(f\"Gradient norm = {b}\\r\", end=\"\")\n",
    "            counter += 1\n",
    "        if counter == max_iter:\n",
    "            print(\"reached max_iter\")\n",
    "            break\n",
    "    return calculate_loss(y, tx, w), w, np.linalg.norm((1/len(y))*calculate_gradient(y, tx, w))\n",
    "\n",
    "\n",
    "def is_cat(x, length=10):\n",
    "    \"\"\"Check if an array is categorical\"\"\"\n",
    "    boolean_index = list([])\n",
    "    if x.shape == (len(x),):\n",
    "        if len(set(x)) < length:\n",
    "            boolean = True\n",
    "        else:\n",
    "            boolean = False\n",
    "            boolean_index.append(boolean)\n",
    "    else:\n",
    "        for i in range(x.shape[1]):\n",
    "            if len(set(x[:, i])) < length:\n",
    "                boolean = True\n",
    "            else:\n",
    "                boolean = False\n",
    "            boolean_index.append(boolean)\n",
    "    return np.array(boolean_index)\n",
    "\n",
    "\n",
    "def build_poly(x, degree, pairwise_interaction=True, intercept=False):\n",
    "    null, x_rNa = row_na_omit(x[:, 0], x)\n",
    "    cat_index = is_cat(x_rNa)\n",
    "    categorical_variables = x[:, cat_index]\n",
    "    continuous_variables = x[:, ~cat_index]\n",
    "    augmented_x = continuous_variables\n",
    "    if degree > 1:\n",
    "        for i in range(2, degree+1):\n",
    "            augmented_x = np.c_[augmented_x, np.power(continuous_variables, i)]\n",
    "    if pairwise_interaction:\n",
    "        for j in tqdm(range(continuous_variables.shape[1])):\n",
    "            for k in range(continuous_variables.shape[1]):\n",
    "                if j >= k:\n",
    "                    continue\n",
    "                else:\n",
    "                    augmented_x = np.c_[augmented_x, np.multiply(\n",
    "                        continuous_variables[:, j], continuous_variables[:, k])]\n",
    "    if intercept:\n",
    "        inter = np.ones((x.shape[0], 1))\n",
    "        augmented_x = np.c_[np.ones((x.shape[0], 1)), augmented_x]\n",
    "    augmented_x = np.c_[augmented_x, categorical_variables]\n",
    "    return augmented_x\n",
    "\n",
    "def threshold(y, fitted_probabilities, step = 0.01):\n",
    "    \"\"\"find the best threshold for classification\"\"\"\n",
    "    candidates = np.arange(0.2, 0.8, step)\n",
    "    thresholds = list([])\n",
    "    accuracies = list([])\n",
    "    for i in tqdm(candidates):\n",
    "        prediction = (fitted_probabilities>i)*1\n",
    "        accuracy = 1 - sum(np.abs(prediction - y))/len(y)\n",
    "        thresholds.append(i)\n",
    "        accuracies.append(accuracy)\n",
    "    index = accuracies.index(max(accuracies))\n",
    "    return thresholds[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pen reg functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_loss(y, tx, w, lambda_=0):\n",
    "    \"\"\" Log-loss for logistic regression \"\"\"\n",
    "    return np.sum(np.log(1. + np.exp(tx.dot(w))) - y * tx.dot(w)) + lambda_ * w[np.newaxis, :].dot(w[:, np.newaxis])[0, 0]\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\" Logistic function \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def logit_gradient(y, tx, w, lambda_=0):\n",
    "    \"\"\" Gradient for logistic loss \"\"\"\n",
    "    return (1/len(y))*tx.T.dot(sigmoid(tx.dot(w)) - y) + 2. * lambda_ * w\n",
    "\n",
    "\n",
    "def logit_stochastic_gradient_descent(y, tx, initial_w, max_iters, gamma, lambda_=0, batch_size=1, eps=1e-4, modulo=100):\n",
    "    \"\"\" Stochastic gradient descent algorithm \"\"\"\n",
    "    for n_iter in range(max_iters):\n",
    "        indices = np.random.permutation(y.shape[0])\n",
    "        stochastic_tx = tx[indices[:batch_size], :]\n",
    "        stochastic_y = y[indices[:batch_size]]\n",
    "        grad = logit_gradient(stochastic_y, stochastic_tx, initial_w, lambda_)\n",
    "        initial_w = initial_w - gamma * grad\n",
    "\n",
    "        if (n_iter % modulo) == 0:\n",
    "            full_grad = logit_gradient(y, tx, initial_w, lambda_)\n",
    "            full_norm = np.linalg.norm(full_grad, np.inf)\n",
    "            print(f\"Gradient norm = {full_norm}\\r\", end=\"\")\n",
    "            if full_norm < eps:\n",
    "                print(f\"                                                \\r\", end=\"\")\n",
    "                print(f\"Stopping criteria reached\")\n",
    "                break\n",
    "\n",
    "    loss = logit_loss(y, tx, initial_w, lambda_)\n",
    "    w = initial_w\n",
    "    grad_norm = np.linalg.norm(logit_gradient(\n",
    "        y, tx, initial_w, lambda_), np.inf)\n",
    "\n",
    "    return loss, w, grad_norm\n",
    "\n",
    "\n",
    "def logit_gradient_descent(y, tx, initial_w, max_iters, gamma, lambda_=0, eps=1e-4):\n",
    "    \"\"\" logit gradient descent algorithm \"\"\"\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        grad = logit_gradient(y, tx, initial_w, lambda_)\n",
    "        initial_w = initial_w - gamma * grad\n",
    "        grad_norm = max(abs(grad))\n",
    "        print(\n",
    "            f\"Gradient norm = {round(grad_norm, 5)} --- {round((n_iter/(max_iters+1))*100, 2)}%         \\r\", end=\"\")\n",
    "\n",
    "        if grad_norm < eps:\n",
    "            print(f\"                                                \\r\", end=\"\")\n",
    "            print(f\"Stopping criteria reached\")\n",
    "            break\n",
    "\n",
    "    loss = logit_loss(y, tx, initial_w, lambda_)\n",
    "    w = initial_w\n",
    "\n",
    "    return loss, w, grad_norm\n",
    "\n",
    "\n",
    "def compute_mse_loss(y, tx, w):\n",
    "    \"\"\" Calculate the MSE loss \"\"\"\n",
    "    return np.sum((y - tx.dot(w))**2) / (2. * y.shape[0])\n",
    "\n",
    "\n",
    "def compute_mae_loss(y, tx, w):\n",
    "    \"\"\" Calculate the MAE loss \"\"\"\n",
    "    return np.sum(np.abs(y - tx.dot(w)) / (2. * y.shape[0]))\n",
    "\n",
    "\n",
    "def compute_logistic_loss(y, tx, w, lambda_=0):\n",
    "    \"\"\" Log-loss for logistic regression \"\"\"\n",
    "    return np.sum(np.log(1. + np.exp(tx.dot(w))) - y * tx.dot(w)) + \\\n",
    "        lambda_ * w[np.newaxis, :].dot(w[:, np.newaxis])[0, 0]\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\" Linear regression using gradient descent \"\"\"\n",
    "    return stochastic_gradient_descent(y, tx, initial_w, max_iters, gamma,\n",
    "                                       compute_mse_loss, compute_mse_gradient)\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\" Linear regression using stochastic gradient descent (mini-batch-size 1) \"\"\"\n",
    "    return stochastic_gradient_descent(y, tx, initial_w, max_iters, gamma,\n",
    "                                       compute_mse_loss, compute_mse_gradient,\n",
    "                                       batch_size=1)\n",
    "\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\" Least squares regression using normal equations \"\"\"\n",
    "    try:\n",
    "        w = np.linalg.solve(tx.T.dot(tx), tx.T.dot(y))\n",
    "    except np.linalg.linalg.LinAlgError:\n",
    "        w = np.linalg.lstsq(tx.T.dot(tx), tx.T.dot(y))[0]\n",
    "    loss = compute_mse_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\" Ridge regression using normal equations \"\"\"\n",
    "    w = np.linalg.solve(tx.T.dot(tx) + lambda_ *\n",
    "                        (2. * y.shape[0]) * np.eye(tx.shape[1]), tx.T.dot(y))\n",
    "    loss = compute_mse_loss(y, tx, w) + \\\n",
    "        lambda_ * w[np.newaxis, :].dot(w[:, np.newaxis])[0, 0]\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\" Logistic regression using gradient descent \"\"\"\n",
    "    return stochastic_gradient_descent(y, tx, initial_w, max_iters, gamma,\n",
    "                                       compute_logistic_loss,\n",
    "                                       compute_logistic_gradient)\n",
    "\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\" Regularized logistic regression using gradient descent \"\"\"\n",
    "    return stochastic_gradient_descent(y, tx, initial_w, max_iters, gamma,\n",
    "                                       compute_logistic_loss,\n",
    "                                       compute_logistic_gradient, lambda_=lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y , x, ids = load_csv_data(\"data/train.csv\")\n",
    "x[np.where(x == -999)] = np.nan\n",
    "y[np.where(y == -1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b2dbeebe7a4b01a323a574943c4f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2052e83d00c44e98b5233430a22e7361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y , x, ids = load_csv_data(\"data/train.csv\")\n",
    "x[np.where(x == -999)] = np.nan\n",
    "y[np.where(y == -1)] = 0\n",
    "\n",
    "y_rNA, x_rNA = row_na_omit(y, x)\n",
    "\n",
    "x_median = median_imputation(x, x_rNA)\n",
    "\n",
    "x_augmented = build_poly(x = x_median, degree = 5, pairwise_interaction = True)\n",
    "x_rNA_augmented = build_poly(x = x_rNA, degree = 5, pairwise_interaction = True)\n",
    "\n",
    "\n",
    "null, mean_, std_ = gaussian_scaling(x_rNA_augmented)\n",
    "x_augmented = (x_augmented - mean_) / std_\n",
    "\n",
    "def eigen(A):\n",
    "    \"\"\"Simple eigenvalue decomposition and sorting by importance\"\"\"\n",
    "    eigenValues, eigenVectors = np.linalg.eig(A)\n",
    "    idx = np.argsort(eigenValues)\n",
    "    eigenValues = eigenValues[idx]\n",
    "    eigenVectors = eigenVectors[:,idx]\n",
    "    return (eigenValues, eigenVectors)\n",
    "\n",
    "cov = np.cov(x_augmented, rowvar=False)\n",
    "eigenvalues, cov_eigenvectors = eigen(cov)\n",
    "\n",
    "x_ort = x_augmented.dot(cov_eigenvectors.real[:,sum(eigenvalues<1e-12):len(eigenvalues)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Penalty = 4.892e-4\n",
    "\n",
    "w_OLS, loss = ridge_regression(y = y, tx = x_ort, lambda_ = Penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping criteria reached                       \n"
     ]
    }
   ],
   "source": [
    "loss, w1, grad_norm = logit_gradient_descent(y = y,\n",
    "                                            tx = x_ort,\n",
    "                                            initial_w = w_OLS,\n",
    "                                            max_iters = 100000,\n",
    "                                            gamma = 0.01,\n",
    "                                            lambda_ = Penalty,\n",
    "                                            eps = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping criteria reached                       \n"
     ]
    }
   ],
   "source": [
    "loss, w2, grad_norm = logit_gradient_descent(y = y,\n",
    "                                            tx = x_ort,\n",
    "                                            initial_w = w1,\n",
    "                                            max_iters = 100000,\n",
    "                                            gamma = 0.01,\n",
    "                                            lambda_ = Penalty,\n",
    "                                            eps = 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping criteria reached                       \n"
     ]
    }
   ],
   "source": [
    "loss, w3, grad_norm = logit_gradient_descent(y = y,\n",
    "                                            tx = x_ort,\n",
    "                                            initial_w = w2,\n",
    "                                            max_iters = 100000,\n",
    "                                            gamma = 0.01,\n",
    "                                            lambda_ = Penalty,\n",
    "                                            eps = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping criteria reached                       \n"
     ]
    }
   ],
   "source": [
    "loss, w4, grad_norm = logit_gradient_descent(y = y,\n",
    "                                            tx = x_ort,\n",
    "                                            initial_w = w3,\n",
    "                                            max_iters = 100000,\n",
    "                                            gamma = 0.01,\n",
    "                                            lambda_ = Penalty,\n",
    "                                            eps = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping criteria reached                       \n"
     ]
    }
   ],
   "source": [
    "loss, w5, grad_norm = logit_gradient_descent(y = y,\n",
    "                                            tx = x_ort,\n",
    "                                            initial_w = w4,\n",
    "                                            max_iters = 100000,\n",
    "                                            gamma = 0.01,\n",
    "                                            lambda_ = Penalty,\n",
    "                                            eps = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm = 0.00154 --- 9.97%         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-460fdce7d764>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m loss, w6, grad_norm = logit_gradient_descent(y = y,\n\u001b[0m\u001b[0;32m      2\u001b[0m                                             \u001b[0mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_ort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                             \u001b[0minitial_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                             \u001b[0mmax_iters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                             \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-f3dfd57af5c2>\u001b[0m in \u001b[0;36mlogit_gradient_descent\u001b[1;34m(y, tx, initial_w, max_iters, gamma, lambda_, eps)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogit_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0minitial_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_w\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-f3dfd57af5c2>\u001b[0m in \u001b[0;36mlogit_gradient\u001b[1;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlogit_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;34m\"\"\" Gradient for logistic loss \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2.\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, w6, grad_norm = logit_gradient_descent(y = y,\n",
    "                                            tx = x_ort,\n",
    "                                            initial_w = w5,\n",
    "                                            max_iters = 100000,\n",
    "                                            gamma = 0.01,\n",
    "                                            lambda_ = Penalty,\n",
    "                                            eps = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wopt = pd.DataFrame(w6)\n",
    "Wopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wopt.to_csv(\"Wopt_ort_aug_clean_data_pol5_pairwise.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700309bd8bff4002b1c50b60c1f8cc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 -- In sample performance : 0.764552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323f7186ab6c4137ab31fc2d2eceec0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 -- In sample performance : 0.728728\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a8f21148384deca468cb712917a2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 -- In sample performance : 0.72966\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114070219f9042b88e79d9214bc58bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 -- In sample performance : 0.733032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f35ffac8c2d4f3abced10b106ba1670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 -- In sample performance : 0.7398359999999999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1d0ac602134d79a2f4cca11ff380b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 -- In sample performance : 0.74942\n"
     ]
    }
   ],
   "source": [
    "W = [w_OLS, w1, w2, w3, w4, w5]\n",
    "for c, w in enumerate(W):\n",
    "    thresh = threshold(y, sigmoid(x_ort@w))\n",
    "    pred = sigmoid(x_ort@w)\n",
    "    pred = (pred>thresh)*1\n",
    "    print(f\"{c} -- In sample performance : {1 - sum(np.abs(pred - y))/len(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.genfromtxt(\"data/W.csv\", delimiter=\",\", skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 484)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ort.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 533 is different from 484)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ce1606eb53e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mthresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_ort\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_ort\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mthresh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{c} -- In sample performance : {1 - sum(np.abs(pred - y))/len(y)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 533 is different from 484)"
     ]
    }
   ],
   "source": [
    "thresh = threshold(y, sigmoid(x_ort@B))\n",
    "pred = sigmoid(x_ort@B)\n",
    "pred = (pred>thresh)*1\n",
    "print(f\"{c} -- In sample performance : {1 - sum(np.abs(pred - y))/len(y)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
