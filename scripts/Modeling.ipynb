{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from extra_helpers import *\n",
    "from feature_importance import *\n",
    "from proj1_helpers import *\n",
    "from data_processing import *\n",
    "from implementations import *\n",
    "from objective_functions import *\n",
    "from run_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 - Data importation.\n",
    "y, tX, ids = load_csv_data(\"data/train.csv\")\n",
    "\n",
    "## 2 - Changing \"-1\" to \"0\" in the response vector\n",
    "##     to be in phase with the major part of the scientific literature.\n",
    "y[np.where(y == -1)] = 0\n",
    "\n",
    "## 3 - Sending the categorical feature (PRI_jet_num) as the last column of the data matrix\n",
    "##     using the rearrange_continuous_categorical_features() function.\n",
    "tX = rearrange_continuous_categorical_features(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 - Knowing that the last column is the categorical feature, we isolate this column\n",
    "##     and identify the indexes for each cases, i.e. PRI_jet_num = 0, 1, 2 or 3.\n",
    "categories = tX[:, -1]\n",
    "zeros_index = np.where(categories == 0)[0]\n",
    "one_index = np.where(categories == 1)[0]\n",
    "two_index = np.where(categories == 2)[0]\n",
    "three_index = np.where(categories == 3)[0]\n",
    "\n",
    "## 2 - We isolate the four different datasets corresponding to the different cases of PRI_jet_num.\n",
    "##   - For each of them:\n",
    "##      - we identify the features having null variance and we delete them.\n",
    "##      - we transform all -999 values to np.nan.\n",
    "##      - we impute the missing values using the median imputation technique.\n",
    "zeros = tX[zeros_index, :]\n",
    "y_zero = y[zeros_index]\n",
    "null_var_index_zero = np.where(np.std(zeros, axis=0) == 0)[0]\n",
    "zeros = np.delete(zeros, null_var_index_zero, axis=1)\n",
    "zeros[np.where(zeros == -999)] = np.nan\n",
    "zeros = median_imputation(zeros)\n",
    "\n",
    "ones = tX[one_index, :]\n",
    "y_one = y[one_index]\n",
    "null_var_index_one = np.where(np.std(ones, axis=0) == 0)[0]\n",
    "ones = np.delete(ones, null_var_index_one, axis=1)\n",
    "ones[np.where(ones == -999)] = np.nan\n",
    "ones = median_imputation(ones)\n",
    "\n",
    "two = tX[two_index, :]\n",
    "y_two = y[two_index]\n",
    "null_var_index_two = np.where(np.std(two, axis=0) == 0)[0]\n",
    "two = np.delete(two, null_var_index_two, axis=1)\n",
    "two[np.where(two == -999)] = np.nan\n",
    "two = median_imputation(two)\n",
    "\n",
    "three = tX[three_index, :]\n",
    "y_three = y[three_index]\n",
    "null_var_index_three = np.where(np.std(three, axis=0) == 0)[0]\n",
    "three = np.delete(three, null_var_index_three, axis=1)\n",
    "three[np.where(three == -999)] = np.nan\n",
    "three = median_imputation(three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the optimal data augmentation for each datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After concluding that the optimal penalty parameter ($\\lambda^{*}_{i}$) for each model were zeros, we lightened the code by deleting this part. For the sake of transparency we will leave visible the code we developed using the golden search algorithm for the model $i = 0$ as a text cell. In the same way, we noted that pairwise interactions were always beneficial."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "accuracies_0 = list()\n",
    "means_0 = list()\n",
    "medians_0 = list()\n",
    "stds_0 = list()\n",
    "lambdas_0 = list()\n",
    "\n",
    "for i in range(1, 21):\n",
    "    print(\"*****************\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Polynomial of degree = {i}\")\n",
    "    tx = process_data(x=zeros, degree=i, pairwise=True, bias=False)\n",
    "    tx, _, __ = gaussian_scaling(tx)\n",
    "    tx, tosolve_tx = orthogonal_basis(tx)\n",
    "    tx = process_data(x=tx, degree=0, pairwise=False, bias=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    def golden_search():\n",
    "        Delta = (3 - np.sqrt(5))/2\n",
    "        counter = 0\n",
    "        ######################\n",
    "        lambda_min = 0\n",
    "        lambda_max = 0.2\n",
    "        eps = 0.001\n",
    "        ######################\n",
    "        while abs(lambda_max - lambda_min) > eps:\n",
    "            L = (lambda_max - lambda_min)\n",
    "            a = lambda_min + Delta*L\n",
    "            b = lambda_max - Delta*L\n",
    "            print(\"\\n\")\n",
    "            print(\"first bound : \")\n",
    "            acc, fa, md, std_ = cross_validation(\n",
    "                y_zero, tx, k_fold=3, lambda_=a)\n",
    "            print(\"\\n\")\n",
    "            print(\"Second bound : \")\n",
    "            acc, fb, md, std_ = cross_validation(\n",
    "                y_zero, tx, k_fold=3, lambda_=b)\n",
    "            if fa < fb:\n",
    "                lambda_min = a\n",
    "            else:\n",
    "                lambda_max = b\n",
    "            print(\"\\n\")\n",
    "            print(f\"Current lambda : {(lambda_min + lambda_max)/2}                 \\r\", end=\"\")\n",
    "            print(\"\\n\")\n",
    "            print(f\"Uncertainty : {np.abs(lambda_min - lambda_max)}                 \\r\", end=\"\")\n",
    "            print(\"\\n\")\n",
    "        return (lambda_min + lambda_max)/2\n",
    "\n",
    "    golden_lambda = golden_search()\n",
    "    if golden_lambda<0.001:\n",
    "        golden_lambda = 0\n",
    "    print(\"\\n\")\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"For polynomial of degree = {i} with golden lambda = {golden_lambda}, we have : \")\n",
    "    acc, m, md, std_ = cross_validation(\n",
    "        y_zero, tx, k_fold=3, lambda_=golden_lambda)\n",
    "    print(\"\\n\")\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    print(\"\\n\")\n",
    "    accuracies_0.append(acc)\n",
    "    means_0.append(m)\n",
    "    medians_0.append(md)\n",
    "    stds_0.append(std_)\n",
    "    lambdas_0.append(golden_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells show how we found the most adequate polynomial degree for each model $i$.\n",
    "\n",
    "The procedure is the following :\n",
    "\n",
    "for $j = 1, 2, ..., 20$ being the degree of the polynomial augmentation:\n",
    "\n",
    "    1 - We augment the data.\n",
    "    2 - We scale the data.\n",
    "    3 - We apply a change of basis.\n",
    "    4 - We add a bias term.\n",
    "    5 - We perform 5-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRI_jet_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means_0 = list()\n",
    "medians_0 = list()\n",
    "stds_0 = list()\n",
    "for i in range(1, 21):\n",
    "    print(\"*****************\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Polynomial of degree = {i}\")\n",
    "    tx = process_data(x=zeros, degree=i, pairwise=True, bias=False) # Data augmentation.\n",
    "    tx, _, __ = gaussian_scaling(tx) # Scaling.\n",
    "    tx, tosolve_tx = orthogonal_basis(tx) # Change of basis.\n",
    "    tx = process_data(x=tx, degree=0, pairwise=False, bias=True) # Adding a bias.\n",
    "    acc, m, md, std_ = cross_validation(y_zero, tx, k_fold=5) # Cross-validating.\n",
    "    means_0.append(m)\n",
    "    medians_0.append(md)\n",
    "    stds_0.append(std_)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRI_jet_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_1 = list()\n",
    "medians_1 = list()\n",
    "stds_1 = list()\n",
    "for i in range(1, 21):\n",
    "    print(\"*****************\")\n",
    "    print(\"\\n\")\n",
    "    print(i)\n",
    "    tx = process_data(x = ones, degree=i, pairwise=True, bias=False) # Data augmentation.\n",
    "    tx, _, __ = gaussian_scaling(tx) # Scaling.\n",
    "    tx, tosolve_tx = orthogonal_basis(tx) # Change of basis.\n",
    "    tx = process_data(x = tx, degree=0, pairwise=False, bias=True) # Adding a bias.\n",
    "    acc, m, md, std_ = cross_validation(y_one, tx, k_fold=5) # Cross-validating.\n",
    "    means_1.append(m)\n",
    "    medians_1.append(md)\n",
    "    stds_1.append(std_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRI_jet_num = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means_2 = list()\n",
    "medians_2 = list()\n",
    "stds_2 = list()\n",
    "for i in range(1, 21):\n",
    "    print(\"*****************\")\n",
    "    print(\"\\n\")\n",
    "    print(i)\n",
    "    tx = process_data(x = two, degree=i, pairwise=True, bias=False) # Data augmentation.\n",
    "    tx, _, __ = gaussian_scaling(tx) # Scaling.\n",
    "    tx, tosolve_tx = orthogonal_basis(tx) # Change of basis.\n",
    "    tx = process_data(x = tx, degree=0, pairwise=False, bias=True) # Adding a bias.\n",
    "    acc, m, md, std_ = cross_validation(y_two, tx, k_fold=5) # Cross-validating.\n",
    "    means_2.append(m)\n",
    "    medians_2.append(md)\n",
    "    stds_2.append(std_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRI_jet_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means_3 = list()\n",
    "medians_3 = list()\n",
    "stds_3 = list()\n",
    "for i in range(1, 21):\n",
    "    print(\"\\n\")\n",
    "    print(\"*****************\")\n",
    "    print(\"\\n\")\n",
    "    print(i)\n",
    "    tx = process_data(x = three, degree=i, pairwise=True, bias=False) # Data augmentation.\n",
    "    tx, _, __ = gaussian_scaling(tx) # Scaling.\n",
    "    tx, tosolve_tx = orthogonal_basis(tx) # Change of basis.\n",
    "    tx = process_data(x = tx, degree=0, pairwise=False, bias=True) # Adding a bias.\n",
    "    acc, m, md, std_ = cross_validation(y_three, tx, k_fold=5) # Cross-validating.\n",
    "    means_3.append(m)\n",
    "    medians_3.append(md)\n",
    "    stds_3.append(std_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the optimal parameters for all four models were determined. In this section, we train the four models starting with the Newton's method (because of the nice convergence of this algorithm) and if the optimization stops prematurely because of singular hessian or because of numerical instabilities, we pursue with a gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model$_{PRI jet num = 0}$ i.e. $w_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We apply the relevant transformations based on optimal parameters found earlier.\n",
    "tx_zeros = process_data(x = zeros, degree=13, pairwise=True, bias=False)\n",
    "tx_zeros, mean_tx_zeros, std_tx_zeros = gaussian_scaling(tx_zeros)\n",
    "tx_zeros, tosolve_tx_zeros = orthogonal_basis(tx_zeros)\n",
    "tx_zeros = process_data(x = tx_zeros, degree=0, pairwise=False, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newton's method & gradient descent.\n",
    "loss_0, w_0, grad_norm_0 = logistic_newton_descent(y_zero,\n",
    "                                                   tx_zeros,\n",
    "                                                   w=np.zeros(\n",
    "                                                       tx_zeros.shape[1]),\n",
    "                                                   lambda_=0,\n",
    "                                                   max_iters=1000,\n",
    "                                                   eps=1e-10,\n",
    "                                                   w_start_OLS=True)\n",
    "loss_0, w_0, grad_norm_0 = logistic_gradient_descent(y_zero,\n",
    "                                                     tx_zeros,\n",
    "                                                     w=w_0,\n",
    "                                                     max_iters=30000,\n",
    "                                                     lambda_=0,\n",
    "                                                     gamma=0.05,\n",
    "                                                     eps=1e-4,\n",
    "                                                     w_start_OLS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 - We search for the optimal threshold that maximize the accuracy.\n",
    "## 2 - We check the in-sample performance of our model.\n",
    "thresh_0 = threshold(y_zero, sigmoid(tx_zeros@w_0))\n",
    "pred = (sigmoid(tx_zeros@w_0) > thresh_0)*1\n",
    "accuracy = 1 - sum(np.abs(pred - y_zero))/len(y_zero)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model$_{PRI jet num = 1}$ i.e. $w_{1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We apply the relevant transformations based on optimal parameters found earlier.\n",
    "tx_ones = process_data(x = ones, degree=17, pairwise=True, bias=False)\n",
    "tx_ones, mean_tx_ones, std_tx_ones = gaussian_scaling(tx_ones)\n",
    "tx_ones, tosolve_tx_ones = orthogonal_basis(tx_ones)\n",
    "tx_ones = process_data(x = tx_ones, degree=0, pairwise=False, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newton's method & gradient descent.\n",
    "loss_1, w_1, grad_norm_1 = logistic_newton_descent(y_one,\n",
    "                                                   tx_ones,\n",
    "                                                   w=np.zeros(\n",
    "                                                       tx_ones.shape[1]),\n",
    "                                                   lambda_=0,\n",
    "                                                   max_iters=1000,\n",
    "                                                   eps=1e-10,\n",
    "                                                   w_start_OLS=True)\n",
    "loss_1, w_1, grad_norm_1 = logistic_gradient_descent(y_one,\n",
    "                                                     tx_ones,\n",
    "                                                     w=w_1,\n",
    "                                                     max_iters=30000,\n",
    "                                                     lambda_=0,\n",
    "                                                     gamma=0.05,\n",
    "                                                     eps=2e-4,\n",
    "                                                     w_start_OLS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 - We search for the optimal threshold that maximize the accuracy.\n",
    "## 2 - We check the in-sample performance of our model.\n",
    "thresh_1 = threshold(y_one, sigmoid(tx_ones@w_1))\n",
    "pred = (sigmoid(tx_ones@w_1) > thresh_1)*1\n",
    "accuracy = 1 - sum(np.abs(pred - y_one))/len(y_one)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model$_{PRI jet num = 2}$ i.e. $w_{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We apply the relevant transformations based on optimal parameters found earlier.\n",
    "tx_two = process_data(x = two, degree=13, pairwise=True, bias=False)\n",
    "tx_two, mean_tx_two, std_tx_two = gaussian_scaling(tx_two)\n",
    "tx_two, tosolve_tx_two = orthogonal_basis(tx_two)\n",
    "tx_two = process_data(x = tx_two, degree=0, pairwise=False, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newton's method & gradient descent.\n",
    "loss_2, w_2, grad_norm_2 = logistic_newton_descent(y_two,\n",
    "                                                   tx_two,\n",
    "                                                   w=np.zeros(\n",
    "                                                       tx_two.shape[1]),\n",
    "                                                   lambda_=0,\n",
    "                                                   max_iters=1000,\n",
    "                                                   eps=1e-10,\n",
    "                                                   w_start_OLS=True)\n",
    "loss_2, w_2, grad_norm_2 = logistic_gradient_descent(y_two,\n",
    "                                                     tx_two,\n",
    "                                                     w=w_2,\n",
    "                                                     max_iters=30000,\n",
    "                                                     lambda_=0,\n",
    "                                                     gamma=0.05,\n",
    "                                                     eps=1e-4,\n",
    "                                                     w_start_OLS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 - We search for the optimal threshold that maximize the accuracy.\n",
    "## 2 - We check the in-sample performance of our model.\n",
    "thresh_2 = threshold(y_two, sigmoid(tx_two@w_2))\n",
    "pred = (sigmoid(tx_two@w_2) > thresh_2)*1\n",
    "accuracy = 1 - sum(np.abs(pred - y_two))/len(y_two)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model$_{PRI jet num = 3}$ i.e. $w_{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We apply the relevant transformations based on optimal parameters found earlier.\n",
    "tx_three = process_data(x = three, degree=10, pairwise=True, bias=False)\n",
    "tx_three, mean_tx_three, std_tx_three = gaussian_scaling(tx_three)\n",
    "tx_three, tosolve_tx_three = orthogonal_basis(tx_three)\n",
    "tx_three = process_data(x = tx_three, degree=0, pairwise=False, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newton's method & gradient descent.\n",
    "loss_3, w_3, grad_norm_3 = logistic_newton_descent(y_three,\n",
    "                                                   tx_three,\n",
    "                                                   w=np.zeros(\n",
    "                                                       tx_three.shape[1]),\n",
    "                                                   lambda_=0,\n",
    "                                                   max_iters=1000,\n",
    "                                                   eps=1e-10,\n",
    "                                                   w_start_OLS=True)\n",
    "loss_3, w_3, grad_norm_3 = logistic_gradient_descent(y_three,\n",
    "                                                     tx_three,\n",
    "                                                     w=w_3,\n",
    "                                                     max_iters=30000,\n",
    "                                                     lambda_=0,\n",
    "                                                     gamma=0.05,\n",
    "                                                     eps=1e-4,\n",
    "                                                     w_start_OLS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 - We search for the optimal threshold that maximize the accuracy.\n",
    "## 2 - We check the in-sample performance of our model.\n",
    "thresh_3 = threshold(y_three, sigmoid(tx_three@w_3))\n",
    "pred = (sigmoid(tx_three@w_3) > thresh_3)*1\n",
    "accuracy = 1 - sum(np.abs(pred - y_three))/len(y_three)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 - Loading the test set.\n",
    "## 2 - Splitting the test set according to PRI_jet_num.\n",
    "_, tX_test, ids_test = load_csv_data(\"data/test.csv\")\n",
    "zeros_test, ones_test, two_test, three_test, zeros_index_test, one_index_test, two_index_test, three_index_test = PRI_jet_num_split(\n",
    "    tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying the relevant transformation found on the training sets to the test sets.\n",
    "##      - Deleting features on the test set based on null variance features of the training sets.\n",
    "##      - Imputing missing values.\n",
    "##      - Data augmentation based on optimal parameters found earlier.\n",
    "##      - Scaling based on training statistics.\n",
    "##      - Change of basis based on training eigenvectors.\n",
    "zeros_test = process_testdata(\n",
    "    zeros_test, null_var_index_zero, 13, mean_tx_zeros, std_tx_zeros, tosolve_tx_zeros)\n",
    "ones_test = process_testdata(\n",
    "    ones_test, null_var_index_one, 17, mean_tx_ones, std_tx_ones, tosolve_tx_ones)\n",
    "two_test = process_testdata(\n",
    "    two_test, null_var_index_two, 13, mean_tx_two, std_tx_two, tosolve_tx_two)\n",
    "three_test = process_testdata(\n",
    "    three_test, null_var_index_three, 10, mean_tx_three, std_tx_three, tosolve_tx_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 - Predicting labels using trained models and optimal thresholds.\n",
    "## 2 - Transforming back \"0\" to \"-1\".\n",
    "## 3 - Mapping the predictions to their original place in the response vector.\n",
    "predictions = _\n",
    "predictions[zeros_index_test] = predict(zeros_test, w_0, thresh_0)\n",
    "predictions[one_index_test] = predict(ones_test, w_1, thresh_1)\n",
    "predictions[two_index_test] = predict(two_test, w_2, thresh_2)\n",
    "predictions[three_index_test] = predict(three_test, w_3, thresh_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.676399677599879"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking the proportion of \"-1\"\n",
    "len(np.where(predictions==-1)[0])/(len(np.where(predictions==-1)[0])+len(np.where(predictions==1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prediction csv file\n",
    "create_csv_submission(ids_test, predictions, \"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
