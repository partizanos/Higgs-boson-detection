{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from proj1_helpers import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0 = np.genfromtxt(\"data/run/w_0.csv\", delimiter=\",\", skip_header=1, dtype=np.complex128)\n",
    "thresh_0 = np.genfromtxt(\"data/run/thresh_0.csv\", delimiter=\",\", skip_header=1)[0]\n",
    "null_var_index_zero = np.genfromtxt(\"data/run/null_var_index_zero.csv\", delimiter=\",\", skip_header=1, dtype=int)\n",
    "mean_tx_zeros = np.genfromtxt(\"data/run/mean_tx_zeros.csv\", delimiter=\",\", skip_header=1)\n",
    "std_tx_zeros = np.genfromtxt(\"data/run/std_tx_zeros.csv\", delimiter=\",\", skip_header=1)\n",
    "tosolve_tx_zeros = np.genfromtxt(\"data/run/tosolve_tx_zeros.csv\", delimiter=\",\", skip_header=1, dtype=np.complex128)\n",
    "\n",
    "w_1 = np.genfromtxt(\"data/run/w_1.csv\", delimiter=\",\", skip_header=1, dtype=np.complex128)\n",
    "thresh_1 = np.genfromtxt(\"data/run/thresh_1.csv\", delimiter=\",\", skip_header=1)[0]\n",
    "null_var_index_one = np.genfromtxt(\"data/run/null_var_index_one.csv\", delimiter=\",\", skip_header=1, dtype=int)\n",
    "mean_tx_ones = np.genfromtxt(\"data/run/mean_tx_ones.csv\", delimiter=\",\", skip_header=1)\n",
    "std_tx_ones = np.genfromtxt(\"data/run/std_tx_ones.csv\", delimiter=\",\", skip_header=1)\n",
    "tosolve_tx_ones = np.genfromtxt(\"data/run/tosolve_tx_ones.csv\", delimiter=\",\", skip_header=1, dtype=np.complex128)\n",
    "\n",
    "w_2 = np.genfromtxt(\"data/run/w_2.csv\", delimiter=\",\", skip_header=1, dtype=np.complex128)\n",
    "thresh_2 = np.genfromtxt(\"data/run/thresh_2.csv\", delimiter=\",\", skip_header=1)[0]\n",
    "null_var_index_two = np.genfromtxt(\"data/run/null_var_index_two.csv\", delimiter=\",\", skip_header=1, dtype=int)\n",
    "mean_tx_two = np.genfromtxt(\"data/run/mean_tx_two.csv\", delimiter=\",\", skip_header=1)\n",
    "std_tx_two = np.genfromtxt(\"data/run/std_tx_two.csv\", delimiter=\",\", skip_header=1)\n",
    "tosolve_tx_two = np.genfromtxt(\"data/run/tosolve_tx_two.csv\", delimiter=\",\", skip_header=1, dtype=np.complex128)\n",
    "\n",
    "w_3 = np.genfromtxt(\"data/run/w_3.csv\", delimiter=\",\", skip_header=1)\n",
    "thresh_3 = np.genfromtxt(\"data/run/thresh_3.csv\", delimiter=\",\", skip_header=1)[0]\n",
    "null_var_index_three = np.genfromtxt(\"data/run/null_var_index_three.csv\", delimiter=\",\", skip_header=1, dtype=int)\n",
    "mean_tx_three = np.genfromtxt(\"data/run/mean_tx_three.csv\", delimiter=\",\", skip_header=1)\n",
    "std_tx_three = np.genfromtxt(\"data/run/std_tx_three.csv\", delimiter=\",\", skip_header=1)\n",
    "tosolve_tx_three = np.genfromtxt(\"data/run/tosolve_tx_three.csv\", delimiter=\",\", skip_header=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, tX_test, ids_test = load_csv_data(\"data/test.csv\")\n",
    "tX_test = rearrange_continuous_categorical_features(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_test = tX_test[:, -1]\n",
    "zeros_index_test = np.where(categories_test == 0)[0]\n",
    "one_index_test = np.where(categories_test == 1)[0]\n",
    "two_index_test = np.where(categories_test == 2)[0]\n",
    "three_index_test = np.where(categories_test == 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \r"
     ]
    }
   ],
   "source": [
    "zeros_test = tX_test[zeros_index_test, :]\n",
    "zeros_test = np.delete(zeros_test, null_var_index_zero, axis=1)\n",
    "zeros_test[np.where(zeros_test == -999)] = np.nan\n",
    "zeros_test = median_imputation(zeros_test)\n",
    "zeros_test = process_data(x = zeros_test, degree=13, pairwise=True, bias=False)\n",
    "zeros_test = (zeros_test - mean_tx_zeros) / std_tx_zeros\n",
    "zeros_test = np.linalg.solve(tosolve_tx_zeros, zeros_test.T).T\n",
    "zeros_test = process_data(x = zeros_test, degree=0, pairwise=False, bias=True)\n",
    "\n",
    "\n",
    "ones_test = tX_test[one_index_test, :]\n",
    "ones_test = np.delete(ones_test, null_var_index_one, axis=1)\n",
    "ones_test[np.where(ones_test == -999)] = np.nan\n",
    "ones_test = median_imputation(ones_test)\n",
    "ones_test = process_data(x = ones_test, degree=17, pairwise=True, bias=False)\n",
    "ones_test = (ones_test - mean_tx_ones) / std_tx_ones\n",
    "ones_test = np.linalg.solve(tosolve_tx_ones, ones_test.T).T\n",
    "ones_test = process_data(x = ones_test, degree=0, pairwise=False, bias=True)\n",
    "\n",
    "two_test = tX_test[two_index_test, :]\n",
    "two_test = np.delete(two_test, null_var_index_two, axis=1)\n",
    "two_test[np.where(two_test == -999)] = np.nan\n",
    "two_test = median_imputation(two_test)\n",
    "two_test = process_data(x = two_test, degree=13, pairwise=True, bias=False)\n",
    "two_test = (two_test - mean_tx_two) / std_tx_two\n",
    "two_test = np.linalg.solve(tosolve_tx_two, two_test.T).T\n",
    "two_test = process_data(x = two_test, degree=0, pairwise=False, bias=True)\n",
    "\n",
    "three_test = tX_test[three_index_test, :]\n",
    "three_test = np.delete(three_test, null_var_index_three, axis=1)\n",
    "three_test[np.where(three_test == -999)] = np.nan\n",
    "three_test = median_imputation(three_test)\n",
    "three_test = process_data(x = three_test, degree=10, pairwise=True, bias=False)\n",
    "three_test = (three_test - mean_tx_three) / std_tx_three\n",
    "three_test = np.linalg.solve(tosolve_tx_three, three_test.T).T\n",
    "three_test = process_data(x = three_test, degree=0, pairwise=False, bias=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhecho Mitev\\OneDrive\\Documents\\EPFL\\Semester 1\\ML\\HBP\\scripts\\functions.py:235: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-x))\n",
      "C:\\Users\\Zhecho Mitev\\OneDrive\\Documents\\EPFL\\Semester 1\\ML\\HBP\\scripts\\functions.py:235: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return 1.0 / (1 + np.exp(-x))\n",
      "<ipython-input-6-c9f7df087648>:2: RuntimeWarning: invalid value encountered in greater\n",
      "  y_pred_zero = (y_pred_zero>thresh_0)*1\n",
      "<ipython-input-6-c9f7df087648>:6: RuntimeWarning: invalid value encountered in greater\n",
      "  y_pred_one = (y_pred_one>thresh_1)*1\n",
      "<ipython-input-6-c9f7df087648>:10: RuntimeWarning: invalid value encountered in greater\n",
      "  y_pred_two = (y_pred_two>thresh_2)*1\n"
     ]
    }
   ],
   "source": [
    "y_pred_zero = sigmoid(zeros_test@w_0)\n",
    "y_pred_zero = (y_pred_zero>thresh_0)*1\n",
    "y_pred_zero[np.where(y_pred_zero == 0)] = -1\n",
    "\n",
    "y_pred_one = sigmoid(ones_test@w_1)\n",
    "y_pred_one = (y_pred_one>thresh_1)*1\n",
    "y_pred_one[np.where(y_pred_one == 0)] = -1\n",
    "\n",
    "y_pred_two = sigmoid(two_test@w_2)\n",
    "y_pred_two = (y_pred_two>thresh_2)*1\n",
    "y_pred_two[np.where(y_pred_two == 0)] = -1\n",
    "\n",
    "y_pred_three = sigmoid(three_test@w_3)\n",
    "y_pred_three = (y_pred_three>thresh_3)*1\n",
    "y_pred_three[np.where(y_pred_three == 0)] = -1\n",
    "\n",
    "predictions = _\n",
    "predictions[zeros_index_test] = y_pred_zero\n",
    "predictions[one_index_test] = y_pred_one\n",
    "predictions[two_index_test] = y_pred_two\n",
    "predictions[three_index_test] = y_pred_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_test) == len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6853853490966814"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(predictions==-1)[0])/(len(np.where(predictions==-1)[0])+len(np.where(predictions==1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution started\n",
      "Importing parameters...\n",
      "Loading testset...\n",
      "Data processing...\n",
      "\n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Predicting...                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonyyazdani/Documents/GitHub/HBP/scripts/functions.py:235: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-x))\n",
      "/Users/anthonyyazdani/Documents/GitHub/HBP/scripts/functions.py:235: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return 1.0 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "### Run file\n",
    "\n",
    "import numpy as np\n",
    "from proj1_helpers import *\n",
    "from functions import *\n",
    "\n",
    "def import_parameters(w_path, threshold_path, null_variance_index_path, mean_path, std_path, eigenvectors_path):\n",
    "    \"\"\"Importing relevant parameters for predictions.\"\"\"\n",
    "    w = np.genfromtxt(w_path, delimiter=\",\", skip_header=1, dtype=np.complex128)\n",
    "    thresh = np.genfromtxt(threshold_path, delimiter=\",\", skip_header=1)[0]\n",
    "    null_var_index = np.genfromtxt(null_variance_index_path, delimiter=\",\", skip_header=1, dtype=int)\n",
    "    mean_tx = np.genfromtxt(mean_path, delimiter=\",\", skip_header=1)\n",
    "    std_tx = np.genfromtxt(std_path, delimiter=\",\", skip_header=1)\n",
    "    tosolve_tx = np.genfromtxt(eigenvectors_path, delimiter=\",\", skip_header=1, dtype=np.complex128)\n",
    "\n",
    "    return w, thresh, null_var_index, mean_tx, std_tx, tosolve_tx\n",
    "\n",
    "def PRI_jet_num_split(data):\n",
    "    \"\"\"Splitting the dataset according to PRI_jet_num which is the categorical feature in our dataset.\"\"\"\n",
    "    tX = rearrange_continuous_categorical_features(data)\n",
    "    categories = tX[:, -1]\n",
    "    zeros_index = np.where(categories == 0)[0]\n",
    "    one_index = np.where(categories == 1)[0]\n",
    "    two_index = np.where(categories == 2)[0]\n",
    "    three_index = np.where(categories == 3)[0]\n",
    "    \n",
    "    zeros = tX[zeros_index, :]\n",
    "    ones = tX[one_index, :]\n",
    "    two = tX[two_index, :]\n",
    "    three = tX[three_index, :]\n",
    "\n",
    "    return zeros, ones, two, three, zeros_index, one_index, two_index, three_index\n",
    "\n",
    "def process_testdata(data, null_var_index, degree, train_mean, train_std, train_eigenvectors):\n",
    "    \"\"\"Apply the necessary transformations to be aligned with the training data.\"\"\"\n",
    "    data = np.delete(data, null_var_index, axis=1)\n",
    "    data[np.where(data == -999)] = np.nan\n",
    "    data = median_imputation(data)\n",
    "    data = process_data(x = data, degree=degree, pairwise=True, bias=False)\n",
    "    data = (data - train_mean) / train_std\n",
    "    data = np.linalg.solve(train_eigenvectors, data.T).T\n",
    "    data = process_data(x = data, degree=0, pairwise=False, bias=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def predict(tx, w, thresh):\n",
    "    pred = sigmoid(tx@w)\n",
    "    pred = (pred>thresh)*1\n",
    "    pred[np.where(pred == 0)] = -1\n",
    "    \n",
    "    return pred\n",
    "\n",
    "# The procedure we use is as follows:\n",
    "#  1 - We locate the data points that corresponds to PRI_jet_num = 0, 1, 2, 3 and we separate them into four datasets.\n",
    "# For each of the datasets:\n",
    "#  2 - We delete the columns whose data are invariant, the same as in the training data.\n",
    "#  3 - We impute the missing values with the median.\n",
    "#  4 - We perform polynomial and pairwise interaction augmentation.\n",
    "#  5 - We scale them using values from the training sets.\n",
    "#  6 - We express them in an orthogonal basis, the same as in the training data.\n",
    "#  7 - We add a bias.\n",
    "\n",
    "# To save time, all needed parameters: \n",
    "\n",
    "#    - w_i : vectors of parameters for model i.\n",
    "#    - thresh_i : the decision threshold to map a probability to {0,1} for model i.\n",
    "#    - null_var_index_i : index that indicates invariant columns for model i.\n",
    "#    - mean_tx_i : vector of means of features for the model i.\n",
    "#    - std_tx_i : vector of standard deviations of features for the model i.\n",
    "#    - tosolve_tx_i : matrix of eigenvectors of features for the model i.\n",
    "\n",
    "# were found earlier and are loaded here.\n",
    "\n",
    "\n",
    "print(\"Execution started\")\n",
    "\n",
    "print(\"Importing parameters...\")\n",
    "w_0, thresh_0, null_var_index_zero, mean_tx_zeros, std_tx_zeros, tosolve_tx_zeros = import_parameters(\"data/run/w_0.csv\", \"data/run/thresh_0.csv\", \"data/run/null_var_index_zero.csv\", \"data/run/mean_tx_zeros.csv\", \"data/run/std_tx_zeros.csv\",\"data/run/tosolve_tx_zeros.csv\")\n",
    "w_1, thresh_1, null_var_index_one, mean_tx_ones, std_tx_ones, tosolve_tx_ones =import_parameters(\"data/run/w_1.csv\", \"data/run/thresh_1.csv\", \"data/run/null_var_index_one.csv\", \"data/run/mean_tx_ones.csv\", \"data/run/std_tx_ones.csv\",\"data/run/tosolve_tx_ones.csv\")\n",
    "w_2, thresh_2, null_var_index_two, mean_tx_two, std_tx_two, tosolve_tx_two = import_parameters(\"data/run/w_2.csv\", \"data/run/thresh_2.csv\", \"data/run/null_var_index_two.csv\", \"data/run/mean_tx_two.csv\", \"data/run/std_tx_two.csv\",\"data/run/tosolve_tx_two.csv\")\n",
    "w_3, thresh_3, null_var_index_three, mean_tx_three, std_tx_three, tosolve_tx_three = import_parameters(\"data/run/w_3.csv\", \"data/run/thresh_3.csv\", \"data/run/null_var_index_three.csv\", \"data/run/mean_tx_three.csv\", \"data/run/std_tx_three.csv\",\"data/run/tosolve_tx_three.csv\")\n",
    "\n",
    "\n",
    "# Predicting\n",
    "\n",
    "print(\"Loading testset...\")\n",
    "_, tX_test, ids_test = load_csv_data(\"data/test.csv\")\n",
    "\n",
    "print(\"Data processing...\")\n",
    "zeros_test, ones_test, two_test, three_test, zeros_index_test, one_index_test, two_index_test, three_index_test = PRI_jet_num_split(tX_test)\n",
    "\n",
    "zeros_test = process_testdata(zeros_test, null_var_index_zero, 13, mean_tx_zeros, std_tx_zeros, tosolve_tx_zeros)\n",
    "ones_test = process_testdata(ones_test, null_var_index_one, 17, mean_tx_ones, std_tx_ones, tosolve_tx_ones)\n",
    "two_test = process_testdata(two_test, null_var_index_two, 13, mean_tx_two, std_tx_two, tosolve_tx_two)\n",
    "three_test = process_testdata(three_test, null_var_index_three, 10, mean_tx_three, std_tx_three, tosolve_tx_three)\n",
    "\n",
    "print(\"Predicting...\")\n",
    "predictions = _\n",
    "predictions[zeros_index_test] = predict(zeros_test, w_0, thresh_0)\n",
    "predictions[one_index_test] = predict(ones_test, w_1, thresh_1)\n",
    "predictions[two_index_test] = predict(two_test, w_2, thresh_2)\n",
    "predictions[three_index_test] = predict(three_test, w_3, thresh_3)\n",
    "\n",
    "create_csv_submission(ids_test, predictions, \"run_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
