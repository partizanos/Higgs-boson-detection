{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from functions import *\n",
    "y, tX, ids = load_csv_data(\"data/train.csv\")\n",
    "tX = np.genfromtxt(\"data/X_train_median_imputed.csv\", delimiter=\",\", skip_header=1)\n",
    "tX = rearrange_continuous_categorical_features(tX)\n",
    "# y, tX = remove_outliers(y, tX, 1.96)\n",
    "y[np.where(y == -1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = build_poly(tX, 10, pairwise_interaction=True, trigo=False, logabs=False, exp=False)\n",
    "tx, _, __ = gaussian_scaling(tx)\n",
    "acc, m, md, std_, seeds = bootstrap_validation(y, tx, repeats=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validation, y_train, y_validation = split_data(y, tx, 0.8, seed=491)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_loss, w, newton_grad_norm = logistic_newton_descent(y_train,\n",
    "                                                           x_train,\n",
    "                                                           w=np.zeros(x_train.shape[1]),\n",
    "                                                           lambda_=0,\n",
    "                                                           max_iters=100,\n",
    "                                                           eps=1e-6,\n",
    "                                                           w_start_OLS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD_loss, w, GD_grad_norm = logistic_gradient_descent(y_train,\n",
    "                                                     x_train,\n",
    "                                                     w=w,\n",
    "                                                     max_iters=1000,\n",
    "                                                     lambda_=0,\n",
    "                                                     gamma=0.05,\n",
    "                                                     eps=1e-4,\n",
    "                                                     w_start_OLS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap validation mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Accuracies = list()\n",
    "for i in range(9,20):\n",
    "    print(\"\\n\")\n",
    "    print(\"*****************\")\n",
    "    print(i)\n",
    "    tx = build_poly(tX, i, pairwise_interaction=True, trigo=False, logabs=False, exp=False)\n",
    "    tx, _, __ = gaussian_scaling(tx)\n",
    "    acc, m, md, std_ = bootstrap_validation(y, tx, repeats=5)\n",
    "    Accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.genfromtxt(\"data/Bootstrap_validation_SKN.csv\", delimiter=\",\", skip_header=1)\n",
    "accuracies = np.array(res[:,2]).reshape((10,19))\n",
    "plt.boxplot(accuracies)\n",
    "plt.ylim(0.75, 0.86)\n",
    "plt.title('Polynomial from 1 to 19 against accuracies \\n Stochastic k-nearest neighbor imputation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.genfromtxt(\"data/Bootstrap_validation_mean.csv\", delimiter=\",\", skip_header=1)\n",
    "accuracies = np.array(res[:,2]).reshape((10,19))\n",
    "plt.boxplot(accuracies)\n",
    "plt.ylim(0.75, 0.86)\n",
    "plt.title('Polynomial from 1 to 19 against accuracies \\n Mean imputation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden search on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Polynomial_degrees = list()\n",
    "Golden_lambdas = list()\n",
    "Accuracies = list()\n",
    "\n",
    "for z in range(1,20):\n",
    "    \n",
    "    tx = median_imputation(tX)\n",
    "    tx = rearrange_continuous_categorical_features(tx)\n",
    "    tx = build_poly(tx, z, pairwise_interaction=True, triplewise_interaction=False, triple_dimensions=False)\n",
    "    tx, mean_, std_ = gaussian_scaling(tx)\n",
    "    tx, todot = eigen(tx)\n",
    "    tx = add_bias(tx)\n",
    "    x_train, x_validation, y_train, y_validation = split_data(y, tx, 0.5, seed=1)\n",
    "\n",
    "    def accuracy(lambda_):\n",
    "        newton_loss, w, newton_grad_norm = logistic_newton_descent(y_train,\n",
    "                                                                   x_train,\n",
    "                                                                   w=np.zeros(x_train.shape[1]),\n",
    "                                                                   lambda_=lambda_,\n",
    "                                                                   max_iters=100,\n",
    "                                                                   eps=1e-6,\n",
    "                                                                   w_start_OLS=True)\n",
    "        GD_loss, w, GD_grad_norm = logistic_gradient_descent(y_train,\n",
    "                                                             x_train,\n",
    "                                                             w=w,\n",
    "                                                             max_iters=500,\n",
    "                                                             lambda_=lambda_,\n",
    "                                                             gamma=0.05,\n",
    "                                                             eps=1e-2,\n",
    "                                                             w_start_OLS=False)\n",
    "\n",
    "        thresh = threshold(y_train, sigmoid(x_train@w))\n",
    "        pred = sigmoid(x_validation@w)\n",
    "        pred = (pred>thresh)*1\n",
    "        return (1 - sum(np.abs(pred - y_validation))/len(y_validation)), w\n",
    "    \n",
    "    def accuracy2():\n",
    "        thresh = threshold(y_train, sigmoid(x_train@w_opt))\n",
    "        pred = sigmoid(x_validation@w_opt)\n",
    "        pred = (pred>thresh)*1\n",
    "        return (1 - sum(np.abs(pred - y_validation))/len(y_validation))\n",
    "\n",
    "    def golden_search():\n",
    "        Delta = (3 - np.sqrt(5))/2\n",
    "        counter = 0\n",
    "        ######################\n",
    "        lambda_min = -10\n",
    "        lambda_max = 1000\n",
    "        eps = 10\n",
    "        ######################\n",
    "        while abs(lambda_max - lambda_min) > eps:\n",
    "            L = (lambda_max - lambda_min)\n",
    "            a = lambda_min + Delta*L\n",
    "            b = lambda_max - Delta*L\n",
    "            counter += 1\n",
    "            fa, w = accuracy(a)\n",
    "            print(f\"Degree {z} polynome progress : {round((counter/20)*100, 2)}%                 \\r\", end=\"\")\n",
    "            counter += 1\n",
    "            fb, w = accuracy(b)\n",
    "            print(f\"Degree {z} polynome progress : {round((counter/20)*100, 2)}%                 \\r\", end=\"\")\n",
    "            if fa < fb:\n",
    "                lambda_min = a\n",
    "            else:\n",
    "                lambda_max = b\n",
    "        return (lambda_min + lambda_max)/2, w\n",
    "\n",
    "    golden_lambda, w_opt = golden_search()\n",
    "    AC = accuracy2()\n",
    "    print(f\"Degree {z} polynome Golden Lambda = {round(golden_lambda, 5)} --- Validation accuracy : {round(AC*100,2)}%\")\n",
    "    Polynomial_degrees.append(z)\n",
    "    Golden_lambdas.append(golden_lambda)\n",
    "    Accuracies.append(AC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden search on cross-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Polynomial_degrees2 = list()\n",
    "Golden_lambdas2 = list()\n",
    "Accuracies2 = list()\n",
    "\n",
    "for z in range(1,20):\n",
    "    \n",
    "    tx = median_imputation(tX)\n",
    "    tx = rearrange_continuous_categorical_features(tx)\n",
    "    tx = build_poly(tx, z, pairwise_interaction=True, triplewise_interaction=False, triple_dimensions=False)\n",
    "    tx, mean_, std_ = gaussian_scaling(tx)\n",
    "    tx, todot = eigen(tx)\n",
    "    tx = add_bias(tx)\n",
    "    x_train, x_validation, y_train, y_validation = split_data(y, tx, 0.5, seed=1)\n",
    "\n",
    "    def cross_loss(lambda_):\n",
    "        newton_loss, w, newton_grad_norm = logistic_newton_descent(y_train,\n",
    "                                                                   x_train,\n",
    "                                                                   w=np.zeros(x_train.shape[1]),\n",
    "                                                                   lambda_=lambda_,\n",
    "                                                                   max_iters=100,\n",
    "                                                                   eps=1e-6,\n",
    "                                                                   w_start_OLS=True)\n",
    "        GD_loss, w, GD_grad_norm = logistic_gradient_descent(y_train,\n",
    "                                                             x_train,\n",
    "                                                             w=w,\n",
    "                                                             max_iters=500,\n",
    "                                                             lambda_=lambda_,\n",
    "                                                             gamma=0.05,\n",
    "                                                             eps=1e-2,\n",
    "                                                             w_start_OLS=False)\n",
    "\n",
    "        return logit_loss(y_validation, x_validation, w, lambda_=lambda_), w\n",
    "    \n",
    "    def accuracy2():\n",
    "        thresh = threshold(y_train, sigmoid(x_train@w_opt))\n",
    "        pred = sigmoid(x_validation@w_opt)\n",
    "        pred = (pred>thresh)*1\n",
    "        return (1 - sum(np.abs(pred - y_validation))/len(y_validation))\n",
    "\n",
    "    def golden_search():\n",
    "        Delta = (3 - np.sqrt(5))/2\n",
    "        counter = 0\n",
    "        ######################\n",
    "        lambda_min = -10\n",
    "        lambda_max = 1000\n",
    "        eps = 10\n",
    "        ######################\n",
    "        while abs(lambda_max - lambda_min) > eps:\n",
    "            L = (lambda_max - lambda_min)\n",
    "            a = lambda_min + Delta*L\n",
    "            b = lambda_max - Delta*L\n",
    "            counter += 1\n",
    "            fa, w = cross_loss(a)\n",
    "            print(f\"Degree {z} polynome progress : {round((counter/20)*100, 2)}%                 \\r\", end=\"\")\n",
    "            counter += 1\n",
    "            fb, w = cross_loss(b)\n",
    "            print(f\"Degree {z} polynome progress : {round((counter/20)*100, 2)}%                 \\r\", end=\"\")\n",
    "            if fa > fb:\n",
    "                lambda_min = a\n",
    "            else:\n",
    "                lambda_max = b\n",
    "        return (lambda_min + lambda_max)/2, w\n",
    "\n",
    "    golden_lambda, w_opt = golden_search()\n",
    "    AC = accuracy2()\n",
    "    print(f\"Degree {z} polynome Golden Lambda = {round(golden_lambda, 5)} --- Validation accuracy : {round(AC*100,2)}%\")\n",
    "    Polynomial_degrees2.append(z)\n",
    "    Golden_lambdas2.append(golden_lambda)\n",
    "    Accuracies2.append(AC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.logspace(-6, -15, 20)\n",
    "acc = list()\n",
    "degree = list()\n",
    "lambdas = list()\n",
    "\n",
    "for z in range(1,20): \n",
    "    \n",
    "    tx = median_imputation(tX)\n",
    "    tx = rearrange_continuous_categorical_features(tx)\n",
    "    tx = build_poly(tx, z, pairwise_interaction=True, triplewise_interaction=False, triple_dimensions=False)\n",
    "    tx, mean_, std_ = gaussian_scaling(tx)\n",
    "    tx = add_bias(tx)\n",
    "    x_train, x_validation, y_train, y_validation = split_data(y, tx, 0.8, seed=1)\n",
    "    \n",
    "    for counter, j in enumerate(grid):\n",
    "        newton_loss, w, newton_grad_norm = logistic_newton_descent(y_train,\n",
    "                                                                       x_train,\n",
    "                                                                       w=np.zeros(x_train.shape[1]),\n",
    "                                                                       lambda_=j,\n",
    "                                                                       max_iters=100,\n",
    "                                                                       eps=1e-6,\n",
    "                                                                       w_start_OLS=True)\n",
    "        thresh = threshold(y_train, sigmoid(x_train@w))\n",
    "        pred = sigmoid(x_validation@w)\n",
    "        pred = (pred>thresh)*1\n",
    "        acc.append((1 - sum(np.abs(pred - y_validation))/len(y_validation)))\n",
    "        degree.append(z)\n",
    "        lambdas.append(grid[counter])\n",
    "        print(f\"Degree {z} polynome --- Lambda = {round(grid[counter])} --- Validation accuracy : {round(acc[counter+z-1]*100,2)}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready to publish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute on all the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from functions import *\n",
    "y, _, ids = load_csv_data(\"data/train.csv\")\n",
    "tX = np.genfromtxt(\"data/X_train_median_imputed.csv\", delimiter=\",\", skip_header=1)\n",
    "tX = rearrange_continuous_categorical_features(tX)\n",
    "y[np.where(y == -1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = build_poly(tX, 9, pairwise_interaction=True, trigo=False, logabs=False, exp=False)\n",
    "tx, x_validation, y, y_validation = split_data(y, tx, 0.8, seed=491)\n",
    "tx, mean_, std_ = gaussian_scaling(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_loss, w, newton_grad_norm = logistic_newton_descent(y,\n",
    "                                                           tx,\n",
    "                                                           w=np.zeros(tx.shape[1]),\n",
    "                                                           lambda_=0,\n",
    "                                                           max_iters=1000,\n",
    "                                                           eps=1e-10,\n",
    "                                                           w_start_OLS=True)\n",
    "\n",
    "GD_loss, w, GD_grad_norm = logistic_gradient_descent(y,\n",
    "                                                     tx,\n",
    "                                                     w=w,\n",
    "                                                     max_iters=1500,\n",
    "                                                     lambda_=0,\n",
    "                                                     gamma=0.05,\n",
    "                                                     eps=1e-4,\n",
    "                                                     w_start_OLS=False)\n",
    "\n",
    "thresh = threshold(y, sigmoid(tx@w))\n",
    "pred = (sigmoid(tx@w) > thresh)*1\n",
    "accuracy = 1 - sum(np.abs(pred - y))/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD_loss, w, GD_grad_norm = logistic_gradient_descent(y,\n",
    "                                                     tx,\n",
    "                                                     w=w,\n",
    "                                                     max_iters=10000,\n",
    "                                                     lambda_=0,\n",
    "                                                     gamma=0.05,\n",
    "                                                     eps=1e-4,\n",
    "                                                     w_start_OLS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = threshold(y_validation, sigmoid(x_validation@w), step=0.001)\n",
    "pred = (sigmoid(x_validation@w) > thresh)*1\n",
    "accuracy = 1 - sum(np.abs(pred - y_validation))/len(y_validation)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from functions import *\n",
    "_, __, ids_test = load_csv_data(\"data/test.csv\")\n",
    "x_test = np.genfromtxt(\"data/X_test_median_imputed.csv\", delimiter=\",\", skip_header=1)\n",
    "x_test = rearrange_continuous_categorical_features(x_test)\n",
    "A = build_poly(x_test, 9, pairwise_interaction=True, trigo=False, logabs=False, exp=False)\n",
    "A = (A - mean_) / std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create prediction\n",
    "y_pred = sigmoid(A@w)\n",
    "y_pred = (y_pred>thresh)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred)/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[np.where(y_pred == 0)] = -1\n",
    "print(len(y_pred[np.where(y_pred == 1)])/len(y_pred[np.where(y_pred == -1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, \"anthony_submission_stochpol9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from functions import *\n",
    "y, _, ids = load_csv_data(\"data/train.csv\")\n",
    "tX = np.genfromtxt(\"data/X_train_median_imputed.csv\", delimiter=\",\", skip_header=1)\n",
    "tx = build_poly(tX, 0, pairwise_interaction=False, triplewise_interaction=False, triple_dimensions=False)\n",
    "tx, _, __ = gaussian_scaling(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y[np.where(y == -1)] = 0\n",
    "dt = np.c_[y, tx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dt).to_csv(\"data/h2o_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = np.genfromtxt(\"data/X_test_median_imputed.csv\", delimiter=\",\", skip_header=1)\n",
    "tx = build_poly(tX, 0, pairwise_interaction=False, triplewise_interaction=False, triple_dimensions=False)\n",
    "tx, _, __ = gaussian_scaling(tx)\n",
    "pd.DataFrame(tx).to_csv(\"data/h2o_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import Perceptron\n",
    "clf = Perceptron(tol=1e-3, random_state=0)\n",
    "clf.fit(tx, y)\n",
    "Perceptron()\n",
    "clf.score(tx, y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "471.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
