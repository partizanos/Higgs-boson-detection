{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from functions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "y, tX, ids = load_csv_data(\"data/train.csv\")\n",
    "y[np.where(y == -1)] = 0\n",
    "tX = rearrange_continuous_categorical_features(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = tX[:, -1]\n",
    "zeros_index = np.where(categories == 0)[0]\n",
    "one_index = np.where(categories == 1)[0]\n",
    "two_index = np.where(categories == 2)[0]\n",
    "three_index = np.where(categories == 3)[0]\n",
    "\n",
    "zeros = tX[zeros_index, :]\n",
    "null_var_index = np.where(np.std(zeros, axis=0) == 0)[0]\n",
    "zeros = np.delete(zeros, null_var_index, axis=1)\n",
    "zeros[np.where(zeros == -999)] = np.nan\n",
    "zeros = median_imputation(zeros)\n",
    "y_zero = y[zeros_index]\n",
    "\n",
    "ones = tX[one_index, :]\n",
    "null_var_index = np.where(np.std(ones, axis=0) == 0)[0]\n",
    "ones = np.delete(ones, null_var_index, axis=1)\n",
    "ones[np.where(ones == -999)] = np.nan\n",
    "ones = median_imputation(ones)\n",
    "y_one = y[one_index]\n",
    "\n",
    "two = tX[two_index, :]\n",
    "null_var_index = np.where(np.std(two, axis=0) == 0)[0]\n",
    "two = np.delete(two, null_var_index, axis=1)\n",
    "two[np.where(two == -999)] = np.nan\n",
    "two = median_imputation(two)\n",
    "y_two = y[two_index]\n",
    "\n",
    "three = tX[three_index, :]\n",
    "null_var_index = np.where(np.std(three, axis=0) == 0)[0]\n",
    "three = np.delete(three, null_var_index, axis=1)\n",
    "three[np.where(three == -999)] = np.nan\n",
    "three = median_imputation(three)\n",
    "y_three = y[three_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means_0 = list()\n",
    "medians_0 = list()\n",
    "stds_0 = list()\n",
    "for i in range(1, 21):\n",
    "    print(\"\\n\")\n",
    "    print(\"*****************\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Polynomial of degree = {i}\")\n",
    "    tx = process_data(x=zeros, degree=i, pairwise=True, bias=False)\n",
    "    tx, _, __ = gaussian_scaling(tx)\n",
    "    tx, tosolve_tx = orthogonal_basis(tx)\n",
    "    tx = process_data(x=tx, degree=0, pairwise=False, bias=True)\n",
    "    acc, m, md, std_ = cross_validation(y_zero, tx, k_fold=5)\n",
    "    means_0.append(m)\n",
    "    medians_0.append(md)\n",
    "    stds_0.append(std_)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracies_0 = list()\n",
    "means_0 = list()\n",
    "medians_0 = list()\n",
    "stds_0 = list()\n",
    "lambdas_0 = list()\n",
    "\n",
    "for i in range(20, 25):\n",
    "    print(\"*****************\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Polynomial of degree = {i}\")\n",
    "    tx = process_data(x=zeros, degree=i, pairwise=True, bias=False)\n",
    "    tx, _, __ = gaussian_scaling(tx)\n",
    "    tx, tosolve_tx = orthogonal_basis(tx)\n",
    "    tx = process_data(x=tx, degree=0, pairwise=False, bias=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    def golden_search():\n",
    "        Delta = (3 - np.sqrt(5))/2\n",
    "        counter = 0\n",
    "        ######################\n",
    "        lambda_min = 0\n",
    "        lambda_max = 0.2\n",
    "        eps = 0.001\n",
    "        ######################\n",
    "        while abs(lambda_max - lambda_min) > eps:\n",
    "            L = (lambda_max - lambda_min)\n",
    "            a = lambda_min + Delta*L\n",
    "            b = lambda_max - Delta*L\n",
    "            print(\"\\n\")\n",
    "            print(\"first bound : \")\n",
    "            acc, fa, md, std_ = cross_validation(\n",
    "                y_zero, tx, k_fold=3, lambda_=a)\n",
    "            print(\"\\n\")\n",
    "            print(\"Second bound : \")\n",
    "            acc, fb, md, std_ = cross_validation(\n",
    "                y_zero, tx, k_fold=3, lambda_=b)\n",
    "            if fa < fb:\n",
    "                lambda_min = a\n",
    "            else:\n",
    "                lambda_max = b\n",
    "            print(\"\\n\")\n",
    "            print(f\"Current lambda : {(lambda_min + lambda_max)/2}                 \\r\", end=\"\")\n",
    "            print(\"\\n\")\n",
    "            print(f\"Uncertainty : {np.abs(lambda_min - lambda_max)}                 \\r\", end=\"\")\n",
    "            print(\"\\n\")\n",
    "        return (lambda_min + lambda_max)/2\n",
    "\n",
    "    golden_lambda = golden_search()\n",
    "    if golden_lambda<0.001:\n",
    "        golden_lambda = 0\n",
    "    print(\"\\n\")\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"For polynomial of degree = {i} with golden lambda = {golden_lambda}, we have : \")\n",
    "    acc, m, md, std_ = cross_validation(\n",
    "        y_zero, tx, k_fold=3, lambda_=golden_lambda)\n",
    "    print(\"\\n\")\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    print(\"\\n\")\n",
    "    accuracies_0.append(acc)\n",
    "    means_0.append(m)\n",
    "    medians_0.append(md)\n",
    "    stds_0.append(std_)\n",
    "    lambdas_0.append(golden_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_0 = np.c_[means_0, medians_0, stds_0]\n",
    "pd.DataFrame(res_0).to_csv(\"data/case_0_CV5_statistics_median_null_var_pairwise_bias_scaling_orth_pol_1_to_20_1e4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_1 = list()\n",
    "medians_1 = list()\n",
    "stds_1 = list()\n",
    "for i in range(1, 21):\n",
    "    print(\"\\n\")\n",
    "    print(\"*****************\")\n",
    "    print(\"\\n\")\n",
    "    print(i)\n",
    "    tx = process_data(x = ones, degree=i, pairwise=True, bias=False)\n",
    "    tx, _, __ = gaussian_scaling(tx)\n",
    "    tx, tosolve_tx = orthogonal_basis(tx)\n",
    "    tx = process_data(x = tx, degree=0, pairwise=False, bias=True)\n",
    "    acc, m, md, std_ = cross_validation(y_one, tx, k_fold=5)\n",
    "    means_1.append(m)\n",
    "    medians_1.append(md)\n",
    "    stds_1.append(std_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = np.c_[means_1, medians_1, stds_1]\n",
    "pd.DataFrame(res_1).to_csv(\"data/case_1_CV5_statistics_median_null_var_pairwise_bias_scaling_orth_pol_1_to_20_1e4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means_2 = list()\n",
    "medians_2 = list()\n",
    "stds_2 = list()\n",
    "for i in range(1, 21):\n",
    "    print(\"\\n\")\n",
    "    print(\"*****************\")\n",
    "    print(\"\\n\")\n",
    "    print(i)\n",
    "    tx = process_data(x = two, degree=i, pairwise=True, bias=False)\n",
    "    tx, _, __ = gaussian_scaling(tx)\n",
    "    tx, tosolve_tx = orthogonal_basis(tx)\n",
    "    tx = process_data(x = tx, degree=0, pairwise=False, bias=True)\n",
    "    acc, m, md, std_ = cross_validation(y_two, tx, k_fold=5)\n",
    "    means_2.append(m)\n",
    "    medians_2.append(md)\n",
    "    stds_2.append(std_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2 = np.c_[means_2, medians_2, stds_2]\n",
    "pd.DataFrame(res_2).to_csv(\"data/case_2_CV5_statistics_median_null_var_pairwise_bias_scaling_orth_pol_1_to_20_1e4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means_3 = list()\n",
    "medians_3 = list()\n",
    "stds_3 = list()\n",
    "for i in range(1, 21):\n",
    "    print(\"\\n\")\n",
    "    print(\"*****************\")\n",
    "    print(\"\\n\")\n",
    "    print(i)\n",
    "    tx = process_data(x = three, degree=i, pairwise=True, bias=False)\n",
    "    tx, _, __ = gaussian_scaling(tx)\n",
    "    tx, tosolve_tx = orthogonal_basis(tx)\n",
    "    tx = process_data(x = tx, degree=0, pairwise=False, bias=True)\n",
    "    acc, m, md, std_ = cross_validation(y_three, tx, k_fold=5)\n",
    "    means_3.append(m)\n",
    "    medians_3.append(md)\n",
    "    stds_3.append(std_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_3 = np.c_[means_3, medians_3, stds_3]\n",
    "pd.DataFrame(res_3).to_csv(\"data/case_3_CV5_statistics_median_null_var_pairwise_bias_scaling_orth_pol_1_to_20_1e4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reloading properly the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 18)\n",
      "(99841, 18)\n",
      "(77544, 22)\n",
      "(77456, 22)\n",
      "(50379, 29)\n",
      "(50263, 29)\n",
      "(22164, 29)\n",
      "(22048, 29)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "from functions import *\n",
    "y, tX, ids = load_csv_data(\"data/train.csv\")\n",
    "y[np.where(y == -1)] = 0\n",
    "tX = rearrange_continuous_categorical_features(tX)\n",
    "\n",
    "categories = tX[:, -1]\n",
    "zeros_index = np.where(categories == 0)[0]\n",
    "one_index = np.where(categories == 1)[0]\n",
    "two_index = np.where(categories == 2)[0]\n",
    "three_index = np.where(categories == 3)[0]\n",
    "\n",
    "zeros = tX[zeros_index, :]\n",
    "y_zero = y[zeros_index]\n",
    "null_var_index_zero = np.where(np.std(zeros, axis=0) == 0)[0]\n",
    "zeros = np.delete(zeros, null_var_index_zero, axis=1)\n",
    "zeros[np.where(zeros == -999)] = np.nan\n",
    "zeros = median_imputation(zeros)\n",
    "# print(zeros.shape)\n",
    "# y_zero, zeros = remove_outliers(y_zero, zeros, quantile=0.5)\n",
    "# print(zeros.shape)\n",
    "\n",
    "ones = tX[one_index, :]\n",
    "y_one = y[one_index]\n",
    "null_var_index_one = np.where(np.std(ones, axis=0) == 0)[0]\n",
    "ones = np.delete(ones, null_var_index_one, axis=1)\n",
    "ones[np.where(ones == -999)] = np.nan\n",
    "ones = median_imputation(ones)\n",
    "# print(ones.shape)\n",
    "# y_one, ones = remove_outliers(y_one, ones, quantile=0.5)\n",
    "# print(ones.shape)\n",
    "\n",
    "two = tX[two_index, :]\n",
    "y_two = y[two_index]\n",
    "null_var_index_two = np.where(np.std(two, axis=0) == 0)[0]\n",
    "two = np.delete(two, null_var_index_two, axis=1)\n",
    "two[np.where(two == -999)] = np.nan\n",
    "two = median_imputation(two)\n",
    "# print(two.shape)\n",
    "# y_two, two = remove_outliers(y_two, two, quantile=0.5)\n",
    "# print(two.shape)\n",
    "\n",
    "three = tX[three_index, :]\n",
    "y_three = y[three_index]\n",
    "null_var_index_three = np.where(np.std(three, axis=0) == 0)[0]\n",
    "three = np.delete(three, null_var_index_three, axis=1)\n",
    "three[np.where(three == -999)] = np.nan\n",
    "three = median_imputation(three)\n",
    "# print(three.shape)\n",
    "# y_three, three = remove_outliers(y_three, three, quantile=0.5)\n",
    "# print(three.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \r"
     ]
    }
   ],
   "source": [
    "tx_zeros = process_data(x = zeros, degree=13, pairwise=True, bias=False)\n",
    "tx_zeros, mean_tx_zeros, std_tx_zeros = gaussian_scaling(tx_zeros)\n",
    "tx_zeros, tosolve_tx_zeros = orthogonal_basis(tx_zeros)\n",
    "tx_zeros = process_data(x = tx_zeros, degree=0, pairwise=False, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm = 9.99e-05                  \r"
     ]
    }
   ],
   "source": [
    "loss_0, w_0, grad_norm_0 = logistic_newton_descent(y_zero,\n",
    "                                                   tx_zeros,\n",
    "                                                   w=np.zeros(\n",
    "                                                       tx_zeros.shape[1]),\n",
    "                                                   lambda_=0,\n",
    "                                                   max_iters=1000,\n",
    "                                                   eps=1e-10,\n",
    "                                                   w_start_OLS=True)\n",
    "loss_0, w_0, grad_norm_0 = logistic_gradient_descent(y_zero,\n",
    "                                                     tx_zeros,\n",
    "                                                     w=w_0,\n",
    "                                                     max_iters=30000,\n",
    "                                                     lambda_=0,\n",
    "                                                     gamma=0.05,\n",
    "                                                     eps=1e-4,\n",
    "                                                     w_start_OLS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8512735249045983"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh_0 = threshold(y_zero, sigmoid(tx_zeros@w_0))\n",
    "pred = (sigmoid(tx_zeros@w_0) > thresh_0)*1\n",
    "accuracy = 1 - sum(np.abs(pred - y_zero))/len(y_zero)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \r"
     ]
    }
   ],
   "source": [
    "tx_ones = process_data(x = ones, degree=17, pairwise=True, bias=False)\n",
    "tx_ones, mean_tx_ones, std_tx_ones = gaussian_scaling(tx_ones)\n",
    "tx_ones, tosolve_tx_ones = orthogonal_basis(tx_ones)\n",
    "tx_ones = process_data(x = tx_ones, degree=0, pairwise=False, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm = 0.0002                    \r"
     ]
    }
   ],
   "source": [
    "loss_1, w_1, grad_norm_1 = logistic_newton_descent(y_one,\n",
    "                                                   tx_ones,\n",
    "                                                   w=np.zeros(\n",
    "                                                       tx_ones.shape[1]),\n",
    "                                                   lambda_=0,\n",
    "                                                   max_iters=1000,\n",
    "                                                   eps=1e-10,\n",
    "                                                   w_start_OLS=True)\n",
    "loss_1, w_1, grad_norm_1 = logistic_gradient_descent(y_one,\n",
    "                                                     tx_ones,\n",
    "                                                     w=w_1,\n",
    "                                                     max_iters=30000,\n",
    "                                                     lambda_=0,\n",
    "                                                     gamma=0.05,\n",
    "                                                     eps=2e-4,\n",
    "                                                     w_start_OLS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8190327411691799"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh_1 = threshold(y_one, sigmoid(tx_ones@w_1))\n",
    "pred = (sigmoid(tx_ones@w_1) > thresh_1)*1\n",
    "accuracy = 1 - sum(np.abs(pred - y_one))/len(y_one)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \r"
     ]
    }
   ],
   "source": [
    "tx_two = process_data(x = two, degree=13, pairwise=True, bias=False)\n",
    "tx_two, mean_tx_two, std_tx_two = gaussian_scaling(tx_two)\n",
    "tx_two, tosolve_tx_two = orthogonal_basis(tx_two)\n",
    "tx_two = process_data(x = tx_two, degree=0, pairwise=False, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm = 0.0001                    \r"
     ]
    }
   ],
   "source": [
    "loss_2, w_2, grad_norm_2 = logistic_newton_descent(y_two,\n",
    "                                                   tx_two,\n",
    "                                                   w=np.zeros(\n",
    "                                                       tx_two.shape[1]),\n",
    "                                                   lambda_=0,\n",
    "                                                   max_iters=1000,\n",
    "                                                   eps=1e-10,\n",
    "                                                   w_start_OLS=True)\n",
    "loss_2, w_2, grad_norm_2 = logistic_gradient_descent(y_two,\n",
    "                                                     tx_two,\n",
    "                                                     w=w_2,\n",
    "                                                     max_iters=30000,\n",
    "                                                     lambda_=0,\n",
    "                                                     gamma=0.05,\n",
    "                                                     eps=1e-4,\n",
    "                                                     w_start_OLS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85448540676044"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh_2 = threshold(y_two, sigmoid(tx_two@w_2))\n",
    "pred = (sigmoid(tx_two@w_2) > thresh_2)*1\n",
    "accuracy = 1 - sum(np.abs(pred - y_two))/len(y_two)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \r"
     ]
    }
   ],
   "source": [
    "tx_three = process_data(x = three, degree=10, pairwise=True, bias=False)\n",
    "tx_three, mean_tx_three, std_tx_three = gaussian_scaling(tx_three)\n",
    "tx_three, tosolve_tx_three = orthogonal_basis(tx_three)\n",
    "tx_three = process_data(x = tx_three, degree=0, pairwise=False, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm = 0.0001                    \r"
     ]
    }
   ],
   "source": [
    "loss_3, w_3, grad_norm_3 = logistic_newton_descent(y_three,\n",
    "                                                   tx_three,\n",
    "                                                   w=np.zeros(\n",
    "                                                       tx_three.shape[1]),\n",
    "                                                   lambda_=0,\n",
    "                                                   max_iters=1000,\n",
    "                                                   eps=1e-10,\n",
    "                                                   w_start_OLS=True)\n",
    "loss_3, w_3, grad_norm_3 = logistic_gradient_descent(y_three,\n",
    "                                                     tx_three,\n",
    "                                                     w=w_3,\n",
    "                                                     max_iters=30000,\n",
    "                                                     lambda_=0,\n",
    "                                                     gamma=0.05,\n",
    "                                                     eps=1e-4,\n",
    "                                                     w_start_OLS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855678519593614"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh_3 = threshold(y_three, sigmoid(tx_three@w_3))\n",
    "pred = (sigmoid(tx_three@w_3) > thresh_3)*1\n",
    "accuracy = 1 - sum(np.abs(pred - y_three))/len(y_three)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, tX_test, ids_test = load_csv_data(\"data/test.csv\")\n",
    "tX_test = rearrange_continuous_categorical_features(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_test = tX_test[:, -1]\n",
    "zeros_index_test = np.where(categories_test == 0)[0]\n",
    "one_index_test = np.where(categories_test == 1)[0]\n",
    "two_index_test = np.where(categories_test == 2)[0]\n",
    "three_index_test = np.where(categories_test == 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \n",
      "Polynomial augmentation progress : 100.0%                 \n",
      "Pairwise interaction progress : 100.0%                 \n",
      "Bias : ✔                                  \r"
     ]
    }
   ],
   "source": [
    "zeros_test = tX_test[zeros_index_test, :]\n",
    "zeros_test = np.delete(zeros_test, null_var_index_zero, axis=1)\n",
    "zeros_test[np.where(zeros_test == -999)] = np.nan\n",
    "zeros_test = median_imputation(zeros_test)\n",
    "zeros_test = process_data(x = zeros_test, degree=13, pairwise=True, bias=False)\n",
    "zeros_test = (zeros_test - mean_tx_zeros) / std_tx_zeros\n",
    "zeros_test = np.linalg.solve(tosolve_tx_zeros, zeros_test.T).T\n",
    "zeros_test = process_data(x = zeros_test, degree=0, pairwise=False, bias=True)\n",
    "\n",
    "\n",
    "ones_test = tX_test[one_index_test, :]\n",
    "ones_test = np.delete(ones_test, null_var_index_one, axis=1)\n",
    "ones_test[np.where(ones_test == -999)] = np.nan\n",
    "ones_test = median_imputation(ones_test)\n",
    "ones_test = process_data(x = ones_test, degree=17, pairwise=True, bias=False)\n",
    "ones_test = (ones_test - mean_tx_ones) / std_tx_ones\n",
    "ones_test = np.linalg.solve(tosolve_tx_ones, ones_test.T).T\n",
    "ones_test = process_data(x = ones_test, degree=0, pairwise=False, bias=True)\n",
    "\n",
    "two_test = tX_test[two_index_test, :]\n",
    "two_test = np.delete(two_test, null_var_index_two, axis=1)\n",
    "two_test[np.where(two_test == -999)] = np.nan\n",
    "two_test = median_imputation(two_test)\n",
    "two_test = process_data(x = two_test, degree=13, pairwise=True, bias=False)\n",
    "two_test = (two_test - mean_tx_two) / std_tx_two\n",
    "two_test = np.linalg.solve(tosolve_tx_two, two_test.T).T\n",
    "two_test = process_data(x = two_test, degree=0, pairwise=False, bias=True)\n",
    "\n",
    "three_test = tX_test[three_index_test, :]\n",
    "three_test = np.delete(three_test, null_var_index_three, axis=1)\n",
    "three_test[np.where(three_test == -999)] = np.nan\n",
    "three_test = median_imputation(three_test)\n",
    "three_test = process_data(x = three_test, degree=10, pairwise=True, bias=False)\n",
    "three_test = (three_test - mean_tx_three) / std_tx_three\n",
    "three_test = np.linalg.solve(tosolve_tx_three, three_test.T).T\n",
    "three_test = process_data(x = three_test, degree=0, pairwise=False, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-73-58e2e6903230>:2: RuntimeWarning: invalid value encountered in greater\n",
      "  y_pred_zero = (y_pred_zero>thresh_0)*1\n"
     ]
    }
   ],
   "source": [
    "y_pred_zero = sigmoid(zeros_test@w_0)\n",
    "y_pred_zero = (y_pred_zero>thresh_0)*1\n",
    "y_pred_zero[np.where(y_pred_zero == 0)] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-74-6442ddc7d4e9>:2: RuntimeWarning: invalid value encountered in greater\n",
      "  y_pred_one = (y_pred_one>thresh_1)*1\n"
     ]
    }
   ],
   "source": [
    "y_pred_one = sigmoid(ones_test@w_1)\n",
    "y_pred_one = (y_pred_one>thresh_1)*1\n",
    "y_pred_one[np.where(y_pred_one == 0)] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-75-dd4d225707cd>:2: RuntimeWarning: invalid value encountered in greater\n",
      "  y_pred_two = (y_pred_two>thresh_2)*1\n"
     ]
    }
   ],
   "source": [
    "y_pred_two = sigmoid(two_test@w_2)\n",
    "y_pred_two = (y_pred_two>thresh_2)*1\n",
    "y_pred_two[np.where(y_pred_two == 0)] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_three = sigmoid(three_test@w_3)\n",
    "y_pred_three = (y_pred_three>thresh_3)*1\n",
    "y_pred_three[np.where(y_pred_three == 0)] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping back to original place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = _\n",
    "predictions[zeros_index_test] = y_pred_zero\n",
    "predictions[one_index_test] = y_pred_one\n",
    "predictions[two_index_test] = y_pred_two\n",
    "predictions[three_index_test] = y_pred_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_test) == len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.685531414653719"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(predictions==-1)[0])/(len(np.where(predictions==-1)[0])+len(np.where(predictions==1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, predictions, \"anthony_submission_0123_complex_1e5_2_mean_outliers_0.5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_zeros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
