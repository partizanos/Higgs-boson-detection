{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "ZHECHO :\n",
    "    1. function \"skewness\" applies ^3 is that correct?, additionally currently the skewness is not applied to the data (see code below)\n",
    "    2. counter added to gradient decent\n",
    "    3. Why values 0 and 1 are more suitable? The helpers use 1 and -1 as well as the competition. Trying linear regression with 1 and -1 values, doesn't work, diverges.\n",
    "    4. I think we are currently applying logistic regression, because of sigmoid function\n",
    "    5. Singular value decomposition doesn't work with the first load\n",
    "    6. Why do we need encoding?\n",
    "    7. The best method turns out to be mean imputation on the test (competition) set, maybe the outliers is the reason\n",
    "    8. Competition insights : Turns out that submitting only \"-1\" entries gives accuracy of 0.66, which means that 2/3 of the data is \"background\" and 1/3 \"signal\". Note that the f1-score for the submission was zero. Imputing the data using the mean value outputs an accuracy of 0.697, which is a slight improvement. F1-score is 0.634. Interestingly, even though the kmeans is more accurate on the validation set, it seems that the on the test data it scores less than 0.697. It seems that the method predicts that there are too many \"signals\" (at least 60k more than anticipated)\n",
    "    9. refactorization of the code will be done at the end, in case you create any implementation try to leave small comments to ensure readability of code\n",
    "    \n",
    "ANTHONY :\n",
    "    05/10/20 :\n",
    "        - is_cat() and build_poly() functions added.\n",
    "        - Small correction in outliers() function (typo: \"x_train\" -> \"x\").\n",
    "        - Putting less useful codes as comments.\n",
    "        - The comments below must be taken into account after\n",
    "          the basic problems have been solved. -> See ZHECHO's comments. \n",
    "        - Implementation of the build_poly() functions at the appropriate place (What is your opinion?).\n",
    "          Because the more we increase the dimensions, the more unstable the gradient descent is\n",
    "          (because of the change of scale). Should we 1. augment the data, 2. scale, 3. PCA?\n",
    "          Or maybe 1. PCA, 2. augment, 3. scale? ...\n",
    "        - We need to create a PCA() function that keeps only the useful sub-linear representation of our data.\n",
    "          In this way, we will have orthogonal dimensions and dimensionality reduction -> faster convergence.\n",
    "          \n",
    "ANTHONY :\n",
    "    06/10/20 :\n",
    "        - threshold() function added.\n",
    "        - Change of strategy : imputation, augmentation, scaling, PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries, functions and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from proj1_helpers import *\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sweetviz as sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_scaling(x):\n",
    "    mean_ = np.mean(x, axis=0)\n",
    "    x_scaled = x - mean_\n",
    "    std_ = np.std(x, axis=0)\n",
    "    x_scaled = x_scaled / std_\n",
    "    return x_scaled, mean_, std_\n",
    "\n",
    "\n",
    "def col_na_omit(x, tol=1):\n",
    "    index = sum(np.isnan(x))/(x.shape[0]) > tol\n",
    "    x = x[:, ~index]\n",
    "    return x\n",
    "\n",
    "\n",
    "def row_na_omit(y, x):\n",
    "    index = np.isnan(x).any(axis=1)\n",
    "    x_no_na = x[~index]\n",
    "    y_no_na = y[~index]\n",
    "    return y_no_na, x_no_na\n",
    "\n",
    "\n",
    "def outliers(y, x, clean_data, quantile=3):\n",
    "    print(f\"*****************************************************\")\n",
    "    print(f\"                                                     \")\n",
    "    print(f\"WARNING : Outliers must be removed before imputation.\")\n",
    "    print(f\"                                                     \")\n",
    "    print(f\"IGNORE : RuntimeWarning: invalid value encountered...\",\n",
    "          \"\\n\", \"This is due to NAs in the data set.\")\n",
    "    print(f\"                                                     \")\n",
    "    print(f\"*****************************************************\")\n",
    "    m = np.mean(clean_data, axis=0)\n",
    "    s = np.std(clean_data, axis=0)\n",
    "    lower = m - quantile * s\n",
    "    upper = m + quantile * s\n",
    "\n",
    "    for i in tqdm(range(clean_data.shape[1])):\n",
    "\n",
    "        index = clean_data[:, i] < lower[i]\n",
    "        clean_data = np.delete(clean_data, index*1, axis=0)\n",
    "\n",
    "        index = x[:, i] < lower[i]\n",
    "        x = np.delete(x, index*1, axis=0)\n",
    "        y = np.delete(y, index*1, axis=0)\n",
    "\n",
    "        index = clean_data[:, i] > upper[i]\n",
    "        clean_data = np.delete(clean_data, index*1, axis=0)\n",
    "\n",
    "        index = x[:, i] > upper[i]\n",
    "        x = np.delete(x, index*1, axis=0)\n",
    "        y = np.delete(y, index*1, axis=0)\n",
    "\n",
    "    return y, x, clean_data\n",
    "\n",
    "\n",
    "def mean_imputation(na_data, clean_data):\n",
    "    null, mean_x, std_x = gaussian_scaling(clean_data)\n",
    "    na_data = (na_data - mean_x)/std_x\n",
    "    na_data = np.nan_to_num(na_data, nan=0)\n",
    "    return(na_data)\n",
    "\n",
    "\n",
    "def median_imputation(na_data, clean_data):\n",
    "    null, mean_x, std_x = gaussian_scaling(clean_data)\n",
    "    na_data = (na_data - mean_x)/std_x\n",
    "    median = np.median(null, axis=0)\n",
    "    for i in range(na_data.shape[1]):\n",
    "        na_data[:, i] = np.nan_to_num(na_data[:, i], nan=median[i])\n",
    "    return(na_data)\n",
    "\n",
    "\n",
    "def skewness(x):\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    std_x = np.std(x, axis=0)\n",
    "    kurt = np.mean(((x - mean_x)/std_x)**3, axis=0)\n",
    "    return kurt\n",
    "\n",
    "\n",
    "def min_max_scaling(x):\n",
    "    min_ = np.min(x, axis=0)\n",
    "    max_ = np.max(x, axis=0)\n",
    "    x_scaled = (x - min_)/(max_ - min_)\n",
    "    return x_scaled, min_, max_\n",
    "\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    num_row = len(y)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    index_split = int(np.floor(ratio * num_row))\n",
    "    index_tr = indices[: index_split]\n",
    "    index_te = indices[index_split:]\n",
    "    # create split\n",
    "    x_tr = x[index_tr]\n",
    "    x_te = x[index_te]\n",
    "    y_tr = y[index_tr]\n",
    "    y_te = y[index_te]\n",
    "    return x_tr, x_te, y_tr, y_te\n",
    "\n",
    "\n",
    "def pairwise(p, q):\n",
    "    return np.sqrt(np.sum((p[:, np.newaxis, :]-q[np.newaxis, :, :])**2, axis=2))\n",
    "\n",
    "\n",
    "def random_sample(x, length):\n",
    "    num_row = x.shape[0]\n",
    "    indices = np.random.permutation(num_row)\n",
    "    sample = x[indices][:length]\n",
    "    return sample\n",
    "\n",
    "\n",
    "def stochastic_kmeans_imputation(na_data, clean_data, neighbors=10, length=1000):\n",
    "    clean_data, mean_x, std_x = gaussian_scaling(clean_data)\n",
    "    na_data = (na_data - mean_x)/std_x\n",
    "    sample = random_sample(x=clean_data, length=length)\n",
    "    for i in tqdm(range(na_data.shape[0])):\n",
    "        condition = np.isnan(na_data[i, :])\n",
    "        if len(condition) > 0:\n",
    "            index = np.where(condition)\n",
    "            candidate = np.delete(na_data[i], index)\n",
    "            neighborhood = np.delete(sample, index, axis=1)\n",
    "            distances = pairwise(candidate.reshape(\n",
    "                (1, len(candidate))), neighborhood)\n",
    "            nearest_index = np.argsort(distances)[0][:neighbors]\n",
    "            na_data[i, index] = np.mean(sample[nearest_index], axis=0)[index]\n",
    "    return(na_data)\n",
    "\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss = (1/len(y)) * y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    return (1/len(y))*np.squeeze(- loss)\n",
    "\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    y = np.array(y)\n",
    "    tx = np.array(tx)\n",
    "    w = np.array(w)\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    grad = tx.T.dot(pred - y)\n",
    "    return (1/len(y))*grad\n",
    "\n",
    "\n",
    "def gradient_descent(y, tx, w, max_iter, gamma, eps=1e-4):\n",
    "    counter = 0\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    b = np.linalg.norm(grad)\n",
    "    while b > eps:\n",
    "        counter += 1\n",
    "        grad = calculate_gradient(y, tx, w)\n",
    "        w -= gamma * grad\n",
    "        b = np.linalg.norm(grad)\n",
    "        print(f\"Gradient norm = {b}\\r\", end=\"\")\n",
    "        if counter == max_iter:\n",
    "            print(\"reached max_iter\")\n",
    "            break\n",
    "    return calculate_loss(y, tx, w), w, np.linalg.norm(grad)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, w, batch_size, max_iter, gamma, eps=1e-4):\n",
    "    counter = 0\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    b = np.linalg.norm(grad)\n",
    "    while b > eps:\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad = calculate_gradient(y_batch, tx_batch, w)\n",
    "            b = np.linalg.norm(grad)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            #print(f\"Gradient norm = {b}\\r\", end=\"\")\n",
    "            counter += 1\n",
    "        if counter == max_iter:\n",
    "            print(\"reached max_iter\")\n",
    "            break\n",
    "    return calculate_loss(y, tx, w), w, np.linalg.norm((1/len(y))*calculate_gradient(y, tx, w))\n",
    "\n",
    "\n",
    "def is_cat(x, length=10):\n",
    "    \"\"\"Check if an array is categorical\"\"\"\n",
    "    boolean_index = list([])\n",
    "    if x.shape == (len(x),):\n",
    "        if len(set(x)) < length:\n",
    "            boolean = True\n",
    "        else:\n",
    "            boolean = False\n",
    "            boolean_index.append(boolean)\n",
    "    else:\n",
    "        for i in range(x.shape[1]):\n",
    "            if len(set(x[:, i])) < length:\n",
    "                boolean = True\n",
    "            else:\n",
    "                boolean = False\n",
    "            boolean_index.append(boolean)\n",
    "    return np.array(boolean_index)\n",
    "\n",
    "\n",
    "def build_poly(x, degree, pairwise_interaction=True, intercept=False):\n",
    "    null, x_rNa = row_na_omit(x[:, 0], x)\n",
    "    cat_index = is_cat(x_rNa)\n",
    "    categorical_variables = x[:, cat_index]\n",
    "    continuous_variables = x[:, ~cat_index]\n",
    "    augmented_x = continuous_variables\n",
    "    if degree > 1:\n",
    "        for i in range(2, degree+1):\n",
    "            augmented_x = np.c_[augmented_x, np.power(continuous_variables, i)]\n",
    "    if pairwise_interaction:\n",
    "        for j in tqdm(range(continuous_variables.shape[1])):\n",
    "            for k in range(continuous_variables.shape[1]):\n",
    "                if j >= k:\n",
    "                    continue\n",
    "                else:\n",
    "                    augmented_x = np.c_[augmented_x, np.multiply(\n",
    "                        continuous_variables[:, j], continuous_variables[:, k])]\n",
    "    if intercept:\n",
    "        inter = np.ones((x.shape[0], 1))\n",
    "        augmented_x = np.c_[np.ones((x.shape[0], 1)), augmented_x]\n",
    "    augmented_x = np.c_[augmented_x, categorical_variables]\n",
    "    return augmented_x\n",
    "\n",
    "def threshold(y, fitted_probabilities, step = 0.01):\n",
    "    \"\"\"find the best threshold for classification\"\"\"\n",
    "    candidates = np.arange(0.2, 0.8, step)\n",
    "    thresholds = list([])\n",
    "    accuracies = list([])\n",
    "    for i in tqdm(candidates):\n",
    "        prediction = (fitted_probabilities>i)*1\n",
    "        accuracy = 1 - sum(np.abs(prediction - y))/len(y)\n",
    "        thresholds.append(i)\n",
    "        accuracies.append(accuracy)\n",
    "    index = accuracies.index(max(accuracies))\n",
    "    return thresholds[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, x_train, ids_train = load_csv_data(\"data/train.csv\")\n",
    "x_train[np.where(x_train == -999)] = np.nan\n",
    "y_train[np.where(y_train == -1)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a clean dataset without missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rNA, x_train_rNA = row_na_omit(y_train, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be done on train and test.\n",
    "x_train = col_na_omit(x_train, tol = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_te, y_train, y_te = split_data(x_train, y_train, 0.8, seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't be done on test set.\n",
    "y_train, x_train, x_train_rNA = outliers(y_train, x_train, x_train_rNA, quantile = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian scaling, imputation and skewness analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_mean = mean_imputation(x_train, x_train_rNA)\n",
    "# x_te_mean = mean_imputation(x_te, x_train_rNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originaly, median_imputation was scaling and imputing. But here I wanted to impute on the original scale.\n",
    "# Hence, I define a knew median_imputation() function.\n",
    "def median_imputation(na_data, clean_data):\n",
    "    median = np.median(clean_data, axis=0)\n",
    "    for i in range(na_data.shape[1]):\n",
    "        na_data[:, i] = np.nan_to_num(na_data[:, i], nan=median[i])\n",
    "    return(na_data)\n",
    "\n",
    "# Imputation on the original scale\n",
    "x_train_median = median_imputation(x_train, x_train_rNA)\n",
    "x_te_median = median_imputation(x_te, x_train_rNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic kmeans imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = stochastic_kmeans_imputation(na_data = x_train, clean_data = x_train_rNA, length=1000)\n",
    "# x_te = stochastic_kmeans_imputation(na_data = x_te, clean_data = x_train_rNA, length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation on the original scale\n",
    "x_train_augmented = build_poly(x = x_train_median, degree = 5, pairwise_interaction = True)\n",
    "x_te_augmented = build_poly(x = x_te_median, degree = 5, pairwise_interaction = True)\n",
    "x_train_rNA_augmented = build_poly(x = x_train_rNA, degree = 5, pairwise_interaction = True)\n",
    " \n",
    "# Scaling\n",
    "null, mean_, std_ = gaussian_scaling(x_train_rNA_augmented)\n",
    "x_train_augmented = (x_train_augmented - mean_) / std_\n",
    "x_te_augmented = (x_te_augmented - mean_) / std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under normality assumption, Gaussian scaling makes sense.\n",
    "# print(np.min(x_train_augmented, axis=0), \"\\n\", \"\\n\", np.max(x_train_augmented, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness(x_train_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density example, symmetry seems to be \"good enough\".\n",
    "# pd.DataFrame(x_train_augmented.T[0]).plot(kind='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda = sv.analyze(pd.concat([pd.DataFrame({\"Prediction\" : y_train}), pd.DataFrame(x_train)], axis=1))\n",
    "# eda.show_html('eda.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depency structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We need SVD for fast convergence.\n",
    "# df = pd.DataFrame(x_train_augmented)\n",
    "# f = plt.figure(figsize=(10, 10))\n",
    "# plt.matshow(df.corr(), fignum=f.number)\n",
    "# plt.xticks(range(df.shape[1]), df.columns, fontsize=10, rotation=45)\n",
    "# plt.yticks(range(df.shape[1]), df.columns, fontsize=10)\n",
    "# cb = plt.colorbar()\n",
    "# cb.ax.tick_params(labelsize=10)\n",
    "# plt.title('Correlation Matrix', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigen(A):\n",
    "    \"\"\"Simple eigenvalue decomposition and sorting by importance\"\"\"\n",
    "    eigenValues, eigenVectors = np.linalg.eig(A)\n",
    "    idx = np.argsort(eigenValues)\n",
    "    eigenValues = eigenValues[idx]\n",
    "    eigenVectors = eigenVectors[:,idx]\n",
    "    return (eigenValues, eigenVectors)\n",
    "\n",
    "cov = np.cov(x_train_augmented, rowvar=False)\n",
    "eigenvalues, cov_eigenvectors = eigen(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_changed = x_train_augmented.dot(cov_eigenvectors.real[:,sum(eigenvalues<1e-2):len(eigenvalues)])\n",
    "x_te_changed = x_te_augmented.dot(cov_eigenvectors.real[:,sum(eigenvalues<1e-2):len(eigenvalues)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, w, grad_norm = gradient_descent(y_train,\n",
    "                                      tx = x_train_changed,\n",
    "                                      w = np.zeros(x_train_changed.shape[1]),\n",
    "                                      max_iter = 10,\n",
    "                                      gamma = 0.05,\n",
    "                                      eps = 1e-4)\n",
    "# print(loss,\"\\n\", \"\\n\", w,\"\\n\", \"\\n\", grad_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = threshold(y_train, sigmoid(x_train_changed@w))\n",
    "pred = sigmoid(x_train_changed@w)\n",
    "pred = (pred>thresh)*1\n",
    "print(f\"In sample performance : {1 - sum(np.abs(pred - y_train))/len(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sigmoid(x_te_changed@w)\n",
    "pred = (pred>thresh)*1\n",
    "print(f\"Out of sample performance : {1 - sum(np.abs(pred - y_te))/len(y_te)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recompute on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y , x, ids = load_csv_data(\"data/train.csv\")\n",
    "x[np.where(x == -999)] = np.nan\n",
    "y[np.where(y == -1)] = 0\n",
    "\n",
    "y_rNA, x_rNA = row_na_omit(y, x)\n",
    "\n",
    "# y, x, x_rNA = outliers(y, x, x_rNA, quantile = 3)\n",
    "\n",
    "x_median = median_imputation(x, x_rNA)\n",
    "\n",
    "x_augmented = build_poly(x = x_median, degree = 5, pairwise_interaction = True)\n",
    "x_rNA_augmented = build_poly(x = x_rNA, degree = 5, pairwise_interaction = True)\n",
    "\n",
    "\n",
    "null, mean_, std_ = gaussian_scaling(x_rNA_augmented)\n",
    "x_augmented = (x_augmented - mean_) / std_\n",
    "\n",
    "cov = np.cov(x_augmented, rowvar=False)\n",
    "eigenvalues, cov_eigenvectors = eigen(cov)\n",
    "\n",
    "x_augmented_changed = x_augmented.dot(cov_eigenvectors.real[:,sum(eigenvalues<1e-2):len(eigenvalues)])\n",
    "\n",
    "loss, w, grad_norm = gradient_descent(y,\n",
    "                                      tx = x_augmented_changed,\n",
    "                                      w = np.zeros(x_augmented_changed.shape[1]),\n",
    "                                      max_iter = 10,\n",
    "                                      gamma = 0.05,\n",
    "                                      eps = 1e-4)\n",
    "\n",
    "thresh = threshold(y, sigmoid(x_augmented_changed@w))\n",
    "pred = sigmoid(x_augmented_changed@w)\n",
    "pred = (pred>thresh)*1\n",
    "print(f\"In sample performance : {1 - sum(np.abs(pred - y))/len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test set\n",
    "y_pred, x_test, ids_test = load_csv_data(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure to apply the same imputaion changes to the test set\n",
    "x_test[np.where(x_test == -999)] = np.nan\n",
    "x_test = median_imputation(x_test, x_train_rNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying singular value decomposition as well (not sure if correct)\n",
    "x_test_augmented = build_poly(x = x_test, degree = 5, pairwise_interaction = True)\n",
    "x_test_augmented = (x_test_augmented - mean_) / std_\n",
    "x_test_changed = x_test_augmented.dot(cov_eigenvectors.real[:,sum(eigenvalues<1e-2):len(eigenvalues)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create prediction\n",
    "y_pred = sigmoid(x_test_changed@w)\n",
    "y_pred = (y_pred>thresh)*1\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert vectors for submition\n",
    "y_pred[np.where(y_pred == 0)] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observe the distribution of the classes\n",
    "print(len(y_pred[np.where(y_pred == 1)])/len(y_pred[np.where(y_pred == -1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, \"anthony_submission_06_10_5.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
